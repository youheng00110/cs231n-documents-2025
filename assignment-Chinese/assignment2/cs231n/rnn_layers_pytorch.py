"""该文件定义了循环神经网络中常用的层类型。
"""
import torch


def affine_forward(x, w, b):
    """计算仿射（全连接）层的前向传播。

    输入x的形状为(N, d_1, ..., d_k)，包含N个样本的小批量，
    其中每个样本x[i]的形状为(d_1, ..., d_k)。我们会将每个输入重塑为维度为D = d_1 * ... * d_k的向量，
    然后将其转换为维度为M的输出向量。

    输入：
    - x: 包含输入数据的torch数组，形状为(N, d_1, ..., d_k)
    - w: 权重的torch数组，形状为(D, M)
    - b: 偏置的torch数组，形状为(M,)

    返回：
    - out: 输出，形状为(N, M)
    """
    # 将输入重塑为二维数组（N, D），然后进行矩阵乘法并加上偏置
    out = x.reshape(x.shape[0], -1) @ w + b
    return out


def rnn_step_forward(x, prev_h, Wx, Wh, b):
    """使用tanh激活函数，对基础RNN的单个时间步执行前向传播。

    输入数据的维度为D，隐藏状态的维度为H，小批量大小为N。

    输入：
    - x: 当前时间步的输入数据，形状为(N, D)
    - prev_h: 上一时间步的隐藏状态，形状为(N, H)
    - Wx: 输入到隐藏连接的权重矩阵，形状为(D, H)
    - Wh: 隐藏到隐藏连接的权重矩阵，形状为(H, H)
    - b: 偏置，形状为(H,)

    返回：
    - next_h: 下一个隐藏状态，形状为(N, H)
    """
    next_h = None
    ##############################################################################
    # 实现基础RNN的单个前向步骤。                                                #
    ##############################################################################
    # 
    ##############################################################################
    #                               你的代码结束                                 #
    ##############################################################################
    return next_h


def rnn_forward(x, h0, Wx, Wh, b):
    """在整个数据序列上运行基础RNN的前向传播。
    
    假设输入序列由T个向量组成，每个向量的维度为D。RNN使用大小为H的隐藏层，
    我们处理包含N个序列的小批量。在RNN前向传播之后，返回所有时间步的隐藏状态。

    输入：
    - x: 整个时间序列的输入数据，形状为(N, T, D)
    - h0: 初始隐藏状态，形状为(N, H)
    - Wx: 输入到隐藏连接的权重矩阵，形状为(D, H)
    - Wh: 隐藏到隐藏连接的权重矩阵，形状为(H, H)
    - b: 偏置，形状为(H,)

    返回：
    - h: 整个时间序列的隐藏状态，形状为(N, T, H)
    """
    h = None
    ##############################################################################
    # 对整个输入数据序列实现基础RNN的前向传播。                                  #
    # 你应该使用上面定义的rnn_step_forward函数。可以使用for循环来帮助计算前向传播。#
    ##############################################################################
    # 
    ##############################################################################
    #                               你的代码结束                                 #
    ##############################################################################
    return h


def word_embedding_forward(x, W):
    """词嵌入的前向传播。
    
    我们处理大小为N的小批量，其中每个序列的长度为T。假设词汇表有V个单词，每个单词
    被分配到一个维度为D的向量。

    输入：
    - x: 整数数组，形状为(N, T)，给出单词的索引。x的每个元素idx必须在0 <= idx < V的范围内。
    - W: 权重矩阵，形状为(V, D)，给出所有单词的词向量。

    返回：
    - out: 数组，形状为(N, T, D)，给出所有输入单词的词向量。
    """
    out = None
    ##############################################################################
    # 实现词嵌入的前向传播。                                                    #
    #                                                                            #
    # 提示：使用Pytorch的数组索引可以在一行内完成。                               #
    ##############################################################################
    # 
    ##############################################################################
    #                               你的代码结束                                 #
    ##############################################################################
    return out


def lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b):
    """LSTM单个时间步的前向传播。

    输入数据的维度为D，隐藏状态的维度为H，我们使用大小为N的小批量。

    注意，此文件中已经为你提供了sigmoid()函数。

    输入：
    - x: 输入数据，形状为(N, D)
    - prev_h: 上一个隐藏状态，形状为(N, H)
    - prev_c: 上一个细胞状态，形状为(N, H)
    - Wx: 输入到隐藏的权重，形状为(D, 4H)
    - Wh: 隐藏到隐藏的权重，形状为(H, 4H)
    - b: 偏置，形状为(4H,)

    返回：
    - next_h: 下一个隐藏状态，形状为(N, H)
    - next_c: 下一个细胞状态，形状为(N, H)
    """
    next_h, next_c = None, None
    #############################################################################
    # 实现LSTM单个时间步的前向传播。                                             #
    # 你可能需要使用上面提供的数值稳定的sigmoid实现。                             #
    #############################################################################
    # 
    ##############################################################################
    #                               你的代码结束                                 #
    ##############################################################################

    return next_h, next_c


def lstm_forward(x, h0, Wx, Wh, b):
    """在整个数据序列上运行LSTM的前向传播。
    
    假设输入序列由T个向量组成，每个向量的维度为D。LSTM使用大小为H的隐藏层，
    我们处理包含N个序列的小批量。在LSTM前向传播之后，返回所有时间步的隐藏状态。

    注意，初始细胞状态作为输入传递，但初始细胞状态被设置为零。另外注意，细胞状态不会被返回；
    它是LSTM的内部变量，不会从外部访问。

    输入：
    - x: 输入数据，形状为(N, T, D)
    - h0: 初始隐藏状态，形状为(N, H)
    - Wx: 输入到隐藏连接的权重，形状为(D, 4H)
    - Wh: 隐藏到隐藏连接的权重，形状为(H, 4H)
    - b: 偏置，形状为(4H,)

    返回：
    - h: 所有序列的所有时间步的隐藏状态，形状为(N, T, H)
    """
    h = None
    #############################################################################
    # 实现LSTM在整个时间序列上的前向传播。                                      #
    # 你应该使用刚刚定义的lstm_step_forward函数。                                #
    #############################################################################
    # 
    ##############################################################################
    #                               你的代码结束                                 #
    ##############################################################################

    return h


def temporal_affine_forward(x, w, b):
    """时间仿射层的前向传播。
    
    输入是一组D维向量，排列成大小为N的时间序列小批量，每个序列的长度为T。
    我们使用仿射函数将这些向量中的每一个转换为维度为M的新向量。

    输入：
    - x: 输入数据，形状为(N, T, D)
    - w: 权重，形状为(D, M)
    - b: 偏置，形状为(M,)

    返回：
    - out: 输出数据，形状为(N, T, M)
    """
    N, T, D = x.shape
    M = b.shape[0]
    # 将输入重塑为(N*T, D)，进行矩阵乘法后重塑为(N, T, M)，再加上偏置
    out = (x.reshape(N * T, D) @ w).reshape(N, T, M) + b
    return out


def temporal_softmax_loss(x, y, mask, verbose=False):
    """用于RNN的时间softmax损失的时间版本。
    
    我们假设在长度为T的时间序列的每个时间步，对大小为V的词汇表进行预测，
    处理大小为N的小批量。输入x给出所有时间步的所有词汇元素的分数，y给出每个时间步的真实元素的索引。
    我们在每个时间步使用交叉熵损失，对所有时间步的损失求和并在小批量上取平均值。

    另外，我们可能希望忽略某些时间步的模型输出，因为不同长度的序列可能被组合成一个小批量并填充了NULL标记。
    可选的mask参数告诉我们哪些元素应该对损失有贡献。

    输入：
    - x: 输入分数，形状为(N, T, V)
    - y: 真实索引，形状为(N, T)，其中每个元素在0 <= y[i, t] < V的范围内
    - mask: 布尔数组，形状为(N, T)，其中mask[i, t]表示x[i, t]的分数是否应该对损失有贡献。

    返回：
    - loss: 标量损失值
    """

    N, T, V = x.shape

    # 将输入、标签和掩码展平为一维
    x_flat = x.reshape(N * T, V)
    y_flat = y.reshape(N * T)
    mask_flat = mask.reshape(N * T)

    # 计算交叉熵损失，不进行归约，然后应用掩码，最后求和并除以小批量大小
    loss = torch.nn.functional.cross_entropy(x_flat, y_flat, reduction='none')
    loss = loss * mask_flat.float()
    loss = loss.sum() / N

    return loss
