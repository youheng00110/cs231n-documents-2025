from builtins import range
import numpy as np


def affine_forward(x, w, b):
    """
    计算仿射（全连接）层的前向传播。

    输入x的形状为(N, d_1, ..., d_k)，包含N个样本的小批量数据，
    其中每个样本x[i]的形状为(d_1, ..., d_k)。我们会将每个输入重塑为
    维度为D = d_1 * ... * d_k的向量，然后将其转换为维度为M的输出向量。

    输入：
    - x: 包含输入数据的numpy数组，形状为(N, d_1, ..., d_k)
    - w: 权重的numpy数组，形状为(D, M)
    - b: 偏置的numpy数组，形状为(M,)

    返回：
    - out: 输出，形状为(N, M)
    - cache: 缓存数据 (x, w, b)
    """
    assert w is not None and b is not None, "affine_forward 收到 None 参数"
    x_reshaped = x.reshape(x.shape[0], -1)  # 将输入重塑为(N, D)
    out = np.dot(x_reshaped, w) + b  # 计算仿射变换
    ###########################################################################
    # 待办：实现仿射层的前向传播。将结果存储在out中。你需要将输入重塑为行向量。    #
    ###########################################################################

    ###########################################################################
    #                             你的代码结束                                 #
    ###########################################################################
    cache = (x, w, b)
    return out, cache


def affine_backward(dout, cache):
    """
    计算仿射层的反向传播。

    输入：
    - dout: 上游梯度，形状为(N, M)
    - cache: 缓存的元组：
      - x: 输入数据，形状为(N, d_1, ..., d_k)
      - w: 权重，形状为(D, M)
      - b: 偏置，形状为(M,)

    返回：
    - dx: 关于x的梯度，形状为(N, d1, ..., d_k)
    - dw: 关于w的梯度，形状为(D, M)
    - db: 关于b的梯度，形状为(M,)
    """
    x, w, b = cache
    x_reshaped = x.reshape(x.shape[0], -1)  # 将输入重塑为(N, D)
    dx, dw, db = np.dot(dout, w.T).reshape(x.shape), np.dot(x_reshaped.T, dout), np.sum(dout, axis=0)
    ###########################################################################
    # 待办：实现仿射层的反向传播。                                             #
    ###########################################################################

    ###########################################################################
    #                             你的代码结束                                #
    ###########################################################################
    return dx, dw, db


def relu_forward(x):
    """
    计算整流线性单元（ReLU）层的前向传播。

    输入：
    - x: 任意形状的输入

    返回：
    - out: 输出，与x形状相同
    - cache: 缓存x
    """
    out = np.maximum(0, x)
    ###########################################################################
    # 待办：实现ReLU的前向传播。                                               #
    ###########################################################################

    ###########################################################################
    #                             你的代码结束                                 #
    ###########################################################################
    cache = x
    return out, cache


def relu_backward(dout, cache):
    """
    计算整流线性单元（ReLU）层的反向传播。

    输入：
    - dout: 上游梯度，任意形状
    - cache: 输入x，与dout形状相同

    返回：
    - dx: 关于x的梯度
    """
    x = cache
    dx, x = dout * (x > 0), cache
    ###########################################################################
    # 待办：实现ReLU的反向传播。                                               #
    ###########################################################################

    ###########################################################################
    #                             你的代码结束                                 #
    ###########################################################################
    return dx


def batchnorm_forward(x, gamma, beta, bn_param):
    """
    批量归一化（batch normalization）的前向传播。

    在训练期间，样本均值和（未校正的）样本方差从批量统计中计算出来，
    并用于归一化输入数据。训练期间，我们还保持每个特征的均值和方差的指数衰减滑动平均值，
    这些平均值用于测试时的数据归一化。

    在每个时间步，我们使用基于动量参数的指数衰减来更新均值和方差的滑动平均值：

    running_mean = momentum * running_mean + (1 - momentum) * sample_mean
    running_var = momentum * running_var + (1 - momentum) * sample_var

    注意，批量归一化的论文建议不同的测试时行为：他们使用大量训练图像计算每个特征的样本均值和方差，
    而不是使用滑动平均值。在本实现中，我们选择使用滑动平均值，因为它们不需要额外的估计步骤；
    torch7的批量归一化实现也使用滑动平均值。

    输入：
    - x: 形状为(N, D)的数据
    - gamma: 形状为(D,)的缩放参数
    - beta: 形状为(D,)的偏移参数
    - bn_param: 包含以下键的字典：
      - mode: 'train'或'test'；必需
      - eps: 数值稳定性常数
      - momentum: 滑动均值/方差的动量常数
      - running_mean: 形状为(D,)的特征滑动均值数组
      - running_var: 形状为(D,)的特征滑动方差数组

    返回：
    - out: 形状为(N, D)的输出
    - cache: 反向传播所需的中间值元组
    """
    mode = bn_param["mode"]
    eps = bn_param.get("eps", 1e-5)
    momentum = bn_param.get("momentum", 0.9)

    N, D = x.shape
    running_mean = bn_param.get("running_mean", np.zeros(D, dtype=x.dtype))
    running_var = bn_param.get("running_var", np.zeros(D, dtype=x.dtype))

    out, cache = None, None
    if mode == "train":
        ############################################################################
        # 待办：实现批量归一化训练时的前向传播。                                      #
        # 使用批量统计计算均值和方差，使用这些统计量归一化输入数据，                    #
        # 并使用gamma和beta对归一化后的数据进行缩放和偏移。                            #
        #                                                                           #
        # 你应将输出存储在变量out中。反向传播所需的任何中间值应存储在cache变量中。      #
        #                                                                           #
        # 你还应使用计算出的样本均值和方差以及动量变量来更新滑动均值和滑动方差，         #
        # 将结果存储在running_mean和running_var变量中。                               #
        #                                                                            #
        # 注意，尽管你需要跟踪滑动方差，但你应基于标准差（方差的平方根）对数据进行归一化！#
        # 参考原始论文（https://arxiv.org/abs/1502.03167）可能会有帮助。              #
        #############################################################################
        pass
        #######################################################################
        #                           你的代码结束                                #
        #######################################################################
    elif mode == "test":
        #######################################################################
        # 待办：实现批量归一化测试时的前向传播。                                #
        # 使用滑动均值和方差归一化输入数据，然后使用gamma和beta进行缩放和偏移。   #
        # 将结果存储在out变量中。                                              #
        #######################################################################
        pass
        #######################################################################
        #                          你的代码结束                                 #
        #######################################################################
    else:
        raise ValueError('Invalid forward batchnorm mode "%s"' % mode)

    # 将更新后的滑动均值存储回bn_param
    bn_param["running_mean"] = running_mean
    bn_param["running_var"] = running_var

    return out, cache


def batchnorm_backward(dout, cache):
    """
    批量归一化的反向传播。

    在本实现中，你应在纸上画出批量归一化的计算图，并通过中间节点反向传播梯度。

    输入：
    - dout: 上游梯度，形状为(N, D)
    - cache: 来自batchnorm_forward的中间值

    返回：
    - dx: 关于输入x的梯度，形状为(N, D)
    - dgamma: 关于缩放参数gamma的梯度，形状为(D,)
    - dbeta: 关于偏移参数beta的梯度，形状为(D,)
    """
    dx, dgamma, dbeta = None, None, None
    ###########################################################################
    # 待办：实现批量归一化的反向传播。将结果存储在dx、dgamma和dbeta变量中。       #
    # 参考原始论文（https://arxiv.org/abs/1502.03167）可能会有帮助。            #
    ###########################################################################

    ###########################################################################
    #                             你的代码结束                                #
    ###########################################################################

    return dx, dgamma, dbeta


def batchnorm_backward_alt(dout, cache):
    """
    批量归一化的替代反向传播实现。

    在本实现中，你应在纸上推导批量归一化反向传播的导数，并尽可能简化。
    你应该能得到一个简单的反向传播表达式。更多提示见jupyter笔记本。

    注意：本实现应预期接收与batchnorm_backward相同的cache变量，
    但可能不会使用cache中的所有值。

    输入/输出：与batchnorm_backward相同
    """
    dx, dgamma, dbeta = None, None, None
    ###########################################################################
    # 待办：实现批量归一化的反向传播。将结果存储在dx、dgamma和dbeta变量中。      #
    #                                                                        #
    # 在计算关于中心化输入的梯度后，你应该能在一个语句中计算关于输入的梯度；      #
    # 我们的实现可以放在一行80个字符内。                                        #
    ###########################################################################

    ###########################################################################
    #                             你的代码结束                                #
    ###########################################################################

    return dx, dgamma, dbeta


def layernorm_forward(x, gamma, beta, ln_param):
    """
    层归一化（layer normalization）的前向传播。

    在训练和测试时，输入数据都按每个数据点进行归一化，然后使用与批量归一化相同的gamma和beta参数进行缩放和偏移。

    注意，与批量归一化不同，层归一化在训练和测试时的行为是相同的，我们不需要跟踪任何滑动平均值。

    输入：
    - x: 形状为(N, D)的数据
    - gamma: 形状为(D,)的缩放参数
    - beta: 形状为(D,)的偏移参数
    - ln_param: 包含以下键的字典：
        - eps: 数值稳定性常数

    返回：
    - out: 形状为(N, D)的输出
    - cache: 反向传播所需的中间值元组
    """
    out, cache = None, None
    eps = ln_param.get("eps", 1e-5)
    ###########################################################################
    # 待办：实现层归一化训练时的前向传播。                                      #
    # 归一化输入数据，并使用gamma和beta对归一化后的数据进行缩放和偏移。          #
    # 提示：这可以通过稍微修改批量归一化的训练时实现，并插入一两行代码来完成。    #
    # 特别是，你能想到任何矩阵变换可以让你复制批量归一化的代码并几乎不做修改吗？  #
    ###########################################################################

    ###########################################################################
    #                             你的代码结束                                #
    ###########################################################################
    return out, cache


def layernorm_backward(dout, cache):
    """
    层归一化的反向传播。

    在本实现中，你可以大量依赖已经完成的批量归一化的工作。

    输入：
    - dout: 上游梯度，形状为(N, D)
    - cache: 来自layernorm_forward的中间值

    返回：
    - dx: 关于输入x的梯度，形状为(N, D)
    - dgamma: 关于缩放参数gamma的梯度，形状为(D,)
    - dbeta: 关于偏移参数beta的梯度，形状为(D,)
    """
    dx, dgamma, dbeta = None, None, None
    ############################################################################
    # 待办：实现层归一化的反向传播。                                             #
    #                                                                          #
    # 提示：这可以通过稍微修改批量归一化的训练时实现来完成。前向传播的提示仍然适用！#
    ############################################################################

    ###########################################################################
    #                             你的代码结束                                #
    ###########################################################################
    return dx, dgamma, dbeta


def dropout_forward(x, dropout_param):
    """
    执行（倒置）dropout的前向传播。

    输入：
    - x: 任意形状的输入数据
    - dropout_param: 包含以下键的字典：
      - p: dropout参数。我们以概率p保留每个神经元的输出。
      - mode: 'test'或'train'。如果是训练模式，则执行dropout；
        如果是测试模式，则仅返回输入。
      - seed: 随机数生成器的种子。传递种子使该函数具有确定性，
        这在梯度检查中是需要的，但在实际网络中不需要。

    输出：
    - out: 与x形状相同的数组。
    - cache: 元组(dropout_param, mask)。在训练模式下，mask是用于乘以输入的dropout掩码；
      在测试模式下，mask为None。

    注意：请实现**倒置**dropout，而不是vanilla版本的dropout。
    详见http://cs231n.github.io/neural-networks-2/#reg。

    注意2：请记住p是**保留**神经元输出的概率；这可能与某些资料相反，在那些资料中p被称为丢弃神经元输出的概率。
    """
    p, mode = dropout_param["p"], dropout_param["mode"]
    if "seed" in dropout_param:
        np.random.seed(dropout_param["seed"])

    mask = None
    out = None

    if mode == "train":
        #######################################################################
        # 待办：实现倒置dropout训练阶段的前向传播。将dropout掩码存储在mask变量中。#
        #######################################################################
        pass
        #######################################################################
        #                           你的代码结束                               #
        #######################################################################
    elif mode == "test":
        #######################################################################
        # 待办：实现倒置dropout测试阶段的前向传播。                             #
        #######################################################################
        pass
        #######################################################################
        #                            你的代码结束                              #
        #######################################################################

    cache = (dropout_param, mask)
    out = out.astype(x.dtype, copy=False)

    return out, cache

def dropout_backward(dout, cache):
    """
    执行（倒置）dropout的反向传播。

    输入：
    - dout: 上游梯度，任意形状
    - cache: 来自dropout_forward的(dropout_param, mask)
    """
    dropout_param, mask = cache
    mode = dropout_param["mode"]

    dx = None
    if mode == "train":
        #######################################################################
        # 待办：实现倒置dropout训练阶段的反向传播                              #
        #######################################################################
        pass
        #######################################################################
        #                          你的代码结束                               #
        #######################################################################
    elif mode == "test":
        dx = dout  # 测试模式下，梯度直接传递
    return dx


def conv_forward_naive(x, w, b, conv_param):
    """
    卷积层前向传播的朴素实现。

    输入包含N个数据点，每个数据点有C个通道，高度H和宽度W。
    我们用F个不同的滤波器对每个输入进行卷积，每个滤波器跨越所有C个通道，
    高度为HH，宽度为WW。

    输入：
    - x: 输入数据，形状为(N, C, H, W)
    - w: 滤波器权重，形状为(F, C, HH, WW)
    - b: 偏置，形状为(F,)
    - conv_param: 包含以下键的字典：
      - 'stride': 水平和垂直方向上相邻感受野之间的像素数（步长）
      - 'pad': 将用于对输入进行零填充的像素数

    在填充过程中，应在输入的高度和宽度轴上对称地（即两侧均等）放置'pad'个零。
    注意不要直接修改原始输入x。

    返回：
    - out: 输出数据，形状为(N, F, H', W')，其中H'和W'由下式给出：
      H' = 1 + (H + 2 * pad - HH) / stride
      W' = 1 + (W + 2 * pad - WW) / stride
    - cache: (x, w, b, conv_param)
    """
    out = None
    ###########################################################################
    # 待办：实现卷积层的前向传播。                                              #
    # 提示：你可以使用np.pad函数进行填充。                                      #
    ###########################################################################

    ###########################################################################
    #                             你的代码结束                                #
    ###########################################################################
    cache = (x, w, b, conv_param)
    return out, cache


def conv_backward_naive(dout, cache):
    """
    卷积层反向传播的朴素实现。

    输入：
    - dout: 上游梯度
    - cache: 如conv_forward_naive中的(x, w, b, conv_param)元组

    返回：
    - dx: 关于x的梯度
    - dw: 关于w的梯度
    - db: 关于b的梯度
    """
    dx, dw, db = None, None, None
    ###########################################################################
    # 待办：实现卷积层的反向传播。                                             #
    ###########################################################################

    ###########################################################################
    #                             你的代码结束                                #
    ###########################################################################
    return dx, dw, db


def max_pool_forward_naive(x, pool_param):
    """
    最大池化层前向传播的朴素实现。

    输入：
    - x: 输入数据，形状为(N, C, H, W)
    - pool_param: 包含以下键的字典：
      - 'pool_height': 每个池化区域的高度
      - 'pool_width': 每个池化区域的宽度
      - 'stride': 相邻池化区域之间的距离

    这里不需要填充，例如你可以假设：
      - (H - pool_height) % stride == 0
      - (W - pool_width) % stride == 0

    返回：
    - out: 输出数据，形状为(N, C, H', W')，其中H'和W'由下式给出：
      H' = 1 + (H - pool_height) / stride
      W' = 1 + (W - pool_width) / stride
    - cache: (x, pool_param)
    """
    out = None
    ###########################################################################
    # 待办：实现最大池化层的前向传播。                                          #
    ###########################################################################

    ###########################################################################
    #                             你的代码结束                                 #
    ###########################################################################
    cache = (x, pool_param)
    return out, cache


def max_pool_backward_naive(dout, cache):
    """
    最大池化层反向传播的朴素实现。

    输入：
    - dout: 上游梯度
    - cache: 前向传播中的(x, pool_param)元组

    返回：
    - dx: 关于x的梯度
    """
    dx = None
    ###########################################################################
    # 待办：实现最大池化层的反向传播。                                          #
    ###########################################################################

    ###########################################################################
    #                             你的代码结束                                #
    ###########################################################################
    return dx


def spatial_batchnorm_forward(x, gamma, beta, bn_param):
    """
    空间批量归一化的前向传播。

    输入：
    - x: 输入数据，形状为(N, C, H, W)
    - gamma: 缩放参数，形状为(C,)
    - beta: 偏移参数，形状为(C,)
    - bn_param: 包含以下键的字典：
      - mode: 'train'或'test'；必需
      - eps: 数值稳定性常数
      - momentum: 滑动均值/方差的动量常数。momentum=0意味着每次都完全丢弃旧信息，
        而momentum=1意味着从不吸收新信息。默认的momentum=0.9在大多数情况下效果很好。
      - running_mean: 形状为(D,)的特征滑动均值数组
      - running_var: 形状为(D,)的特征滑动方差数组

    返回：
    - out: 输出数据，形状为(N, C, H, W)
    - cache: 反向传播所需的值
    """
    out, cache = None, None

    ###########################################################################
    # 待办：实现空间批量归一化的前向传播。                                      #
    #                                                                         #
    # 提示：你可以通过调用上面实现的普通版本的批量归一化来实现空间批量归一化。    #
    # 你的实现应该非常简短；我们的实现不到五行。                                #
    ###########################################################################

    ###########################################################################
    #                             你的代码结束                                #
    ###########################################################################

    return out, cache


def spatial_batchnorm_backward(dout, cache):
    """
    空间批量归一化的反向传播。

    输入：
    - dout: 上游梯度，形状为(N, C, H, W)
    - cache: 前向传播中的值

    返回：
    - dx: 关于输入的梯度，形状为(N, C, H, W)
    - dgamma: 关于缩放参数的梯度，形状为(C,)
    - dbeta: 关于偏移参数的梯度，形状为(C,)
    """
    dx, dgamma, dbeta = None, None, None

    ###########################################################################
    # 待办：实现空间批量归一化的反向传播。                                      #
    #                                                                         #
    # 提示：你可以通过调用上面实现的普通版本的批量归一化来实现空间批量归一化。    #
    # 你的实现应该非常简短；我们的实现不到五行。                                #
    ###########################################################################

    ###########################################################################
    #                             你的代码结束                                #
    ###########################################################################

    return dx, dgamma, dbeta


def spatial_groupnorm_forward(x, gamma, beta, G, gn_param):
    """
    空间组归一化的前向传播。
    与层归一化不同，组归一化将数据中的每个条目分成G个连续的部分，然后独立地对每个部分进行归一化。
    然后对数据应用每个特征的偏移和缩放，方式与批量归一化和层归一化相同。

    输入：
    - x: 输入数据，形状为(N, C, H, W)
    - gamma: 缩放参数，形状为(1, C, 1, 1)
    - beta: 偏移参数，形状为(1, C, 1, 1)
    - G: 要分成的组数，应为C的约数
    - gn_param: 包含以下键的字典：
      - eps: 数值稳定性常数

    返回：
    - out: 输出数据，形状为(N, C, H, W)
    - cache: 反向传播所需的值
    """
    out, cache = None, None
    eps = gn_param.get("eps", 1e-5)
    ###########################################################################
    # 待办：实现空间组归一化的前向传播。                                        #
    # 这将与层归一化的实现极其相似。                                            #
    # 特别是，思考如何转换矩阵，使得大部分代码与训练时的批量归一化和层归一化相似！ #
    ###########################################################################

    ###########################################################################
    #                             你的代码结束                                #
    ###########################################################################
    return out, cache


def spatial_groupnorm_backward(dout, cache):
    """
    空间组归一化的反向传播。

    输入：
    - dout: 上游梯度，形状为(N, C, H, W)
    - cache: 前向传播中的值

    返回：
    - dx: 关于输入的梯度，形状为(N, C, H, W)
    - dgamma: 关于缩放参数的梯度，形状为(1, C, 1, 1)
    - dbeta: 关于偏移参数的梯度，形状为(1, C, 1, 1)
    """
    dx, dgamma, dbeta = None, None, None

    ###########################################################################
    # 待办：实现空间组归一化的反向传播。                                        #
    # 这将与层归一化的实现极其相似。                                            #
    ###########################################################################

    ###########################################################################
    #                             你的代码结束                                #
    ###########################################################################
    return dx, dgamma, dbeta


def svm_loss(x, y):
    """
    计算多类SVM分类的损失和梯度。

    输入：
    - x: 输入数据，形状为(N, C)，其中x[i, j]是第i个输入对第j类的得分
    - y: 标签向量，形状为(N,)，其中y[i]是x[i]的标签，且0 <= y[i] < C

    返回：
    - loss: 损失标量
    - dx: 损失关于x的梯度
    """
    loss, dx = None, None

    ###########################################################################
    # 待办：从A1中复制你的解决方案。
    ###########################################################################

    ###########################################################################
    #                             你的代码结束                                #
    ###########################################################################
    return loss, dx


def softmax_loss(x, y):
    """
    计算softmax分类的损失和梯度。

    输入：
    - x: 输入数据，形状为(N, C)，其中x[i, j]是第i个输入对第j类的得分
    - y: 标签向量，形状为(N,)，其中y[i]是x[i]的标签，且0 <= y[i] < C

    返回：
    - loss: 损失标量
    - dx: 损失关于x的梯度
    """
    x_shifted = x - np.max(x, axis=1, keepdims=True)
    x_probs = np.exp(x_shifted) / np.sum((np.exp(x_shifted)), axis=1, keepdims=True)
    loss = -np.sum(np.log(x_probs[np.arange(x.shape[0]), y])) / x.shape[0]
    dx = x_probs.copy()
    dx[np.arange(x.shape[0]), y] -= 1
    dx /= x.shape[0]
   

    ###########################################################################
    # 待办：从A1中复制你的解决方案。
    ###########################################################################

    ###########################################################################
    #                             你的代码结束                                #
    ###########################################################################
    return loss, dx
