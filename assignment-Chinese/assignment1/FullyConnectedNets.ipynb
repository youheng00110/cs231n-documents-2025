{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_Ke3JSAu-pN"
   },
   "outputs": [],
   "source": [
    "# 将 Google Drive 挂载到 Colab 虚拟机，使云端硬盘像本地目录一样可用\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 代办: 请填写你在 Google Drive 中存放解压后作业文件夹的路径\n",
    "# 示例：'cs231n/assignments/assignment1/'\n",
    "# 请根据你的实际路径进行修改\n",
    "FOLDERNAME = 'cs231n/assignments/assignment1/'\n",
    "assert FOLDERNAME is not None, \"[!] 请填写正确的文件夹路径。\"\n",
    "\n",
    "# 挂载完成后，把该路径加入 Python 的模块搜索路径，\n",
    "# 确保 Colab 虚拟机的解释器能找到并加载其中的 .py 文件\n",
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
    "\n",
    "# 如果 CIFAR-10 数据集尚未存在，则将其下载到 Google Drive\n",
    "# 进入数据集目录\n",
    "%cd /content/drive/My\\ Drive/$FOLDERNAME/cs231n/datasets/\n",
    "# 执行下载脚本，自动下载并解压 CIFAR-10\n",
    "!bash get_datasets.sh\n",
    "# 下载完成后返回作业根目录\n",
    "%cd /content/drive/My\\ Drive/$FOLDERNAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hmW4qyEu-pR"
   },
   "source": [
    "# 多层全连接网络  \n",
    "在本练习中，你将实现一个可以包含任意数量隐藏层的全连接神经网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qn88dLWLA3np"
   },
   "outputs": [],
   "source": [
    "# 这两行代码用于在 Google Colab 中挂载 Google Drive。\n",
    "# 取消注释后即可将云端硬盘挂载到 /content/drive，方便读写文件。\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "audT-ccNu-pT"
   },
   "source": [
    "阅读文件 `cs231n/classifiers/fc_net.py` 中的 `FullyConnectedNet` 类。  \n",
    "\n",
    "实现网络的初始化、前向传播和反向传播。  \n",
    "在整个作业过程中，你将在 `cs231n/layers.py` 中逐步完成各个层的实现。  \n",
    "你可以复用之前已经写好的 `affine_forward`、`affine_backward`、`relu_forward`、`relu_backward` 以及 `softmax_loss` 等函数。  \n",
    "目前暂时不需要实现 dropout 或 batch/layer normalization，稍后会要求你添加这些功能。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vYqS6VtUu-pU",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# ========== 初始化设置单元 ==========\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# 从 cs231n 导入将要测试 / 训练的多层全连接网络及相关工具\n",
    "from cs231n.classifiers.fc_net import *\n",
    "from cs231n.data_utils import get_CIFAR10_data          # 快速加载 CIFAR-10 的辅助函数\n",
    "from cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from cs231n.solver import Solver                        # 通用训练器（封装了训练循环）\n",
    "\n",
    "# 让 matplotlib 在 notebook 内嵌显示图形\n",
    "%matplotlib inline\n",
    "# 统一设置图片大小、插值方式和灰度图配色，方便观察\n",
    "plt.rcParams[\"figure.figsize\"] = (10.0, 8.0)\n",
    "plt.rcParams[\"image.interpolation\"] = \"nearest\"\n",
    "plt.rcParams[\"image.cmap\"] = \"gray\"\n",
    "\n",
    "# 启用 IPython 的自动重载功能：修改 .py 文件后无需重启 kernel 即可生效\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# 计算两个矩阵/向量之间的相对误差，常用于数值梯度校验\n",
    "def rel_error(x, y):\n",
    "    \"\"\"返回相对误差。\"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56aAEw7Ku-pU"
   },
   "outputs": [],
   "source": [
    "# 加载已经预处理过的 CIFAR-10 数据集\n",
    "data = get_CIFAR10_data()\n",
    "\n",
    "# 打印各数据子集的名称与形状\n",
    "for k, v in list(data.items()):\n",
    "    print(f\"{k}: {v.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L60FVoXSu-pV"
   },
   "source": [
    "## 初始损失与梯度检查\n",
    "\n",
    "作为完整性检查，请运行以下代码来：\n",
    "\n",
    "1. 查看网络的初始损失值是否合理；\n",
    "2. 对网络进行梯度检查，分别测试 **不含正则化** 和 **含正则化** 的情况。\n",
    "\n",
    "在进行梯度检查时，你应期望得到的相对误差在 **1e-7 或更小**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffLH91h4u-pV"
   },
   "outputs": [],
   "source": [
    "# 设置随机种子，确保结果可复现\n",
    "np.random.seed(231)\n",
    "# 构造一个极小批次：2 张图像、每幅 15 维特征、两层隐藏层分别为 20 和 30 个神经元、10 类\n",
    "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
    "X = np.random.randn(N, D)          # 随机生成输入数据\n",
    "y = np.random.randint(C, size=(N,))  # 随机生成类别标签\n",
    "\n",
    "# 分别测试无正则化和有正则化两种情况\n",
    "for reg in [0, 3.14]:\n",
    "    print(\"正在测试，reg =\", reg)\n",
    "    \n",
    "    # 构建两层隐藏层的全连接网络\n",
    "    model = FullyConnectedNet(\n",
    "        [H1, H2],\n",
    "        input_dim=D,\n",
    "        num_classes=C,\n",
    "        reg=reg,\n",
    "        weight_scale=5e-2,\n",
    "        dtype=np.float64\n",
    "    )\n",
    "\n",
    "    # 计算初始损失及梯度\n",
    "    loss, grads = model.loss(X, y)\n",
    "    print(\"初始损失:\", loss)\n",
    "\n",
    "    # 对每一组参数进行数值梯度检查\n",
    "    # 大部分相对误差应 ≤ 1e-7；当 reg=0 时，W2 的误差允许到 1e-5 左右\n",
    "    for name in sorted(grads):\n",
    "        f = lambda _: model.loss(X, y)[0]   # 只返回损失的匿名函数\n",
    "        grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)\n",
    "        print(f\"{name} 相对误差: {rel_error(grad_num, grads[name])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSiVnvNgu-pW"
   },
   "source": [
    "作为另一项完整性检查，请确保你的网络可以在 **仅含 50 张图像的小数据集上实现过拟合**。  \n",
    "我们将先采用一个三层网络，每个隐藏层 100 个神经元。  \n",
    "\n",
    "在接下来的代码单元中，请调整 **学习率 (learning rate)** 和 **权重初始化尺度 (weight initialization scale)**，  \n",
    "使得模型在 **20 个 epoch 以内** 达到 **100% 的训练准确率**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HYHs3IPmu-pW",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO：仅用三层网络（每层 100 个神经元）在 50 个训练样本上过拟合，\n",
    "# 只需调节学习率和权重初始化尺度即可。\n",
    "\n",
    "num_train = 50  # 仅取前 50 张图片作为极小训练集\n",
    "small_data = {\n",
    "    \"X_train\": data[\"X_train\"][:num_train],\n",
    "    \"y_train\": data[\"y_train\"][:num_train],\n",
    "    \"X_val\":   data[\"X_val\"],\n",
    "    \"y_val\":   data[\"y_val\"],\n",
    "}\n",
    "\n",
    "# 以下两个超参数需要你手动尝试不同数值，以达到 20 个 epoch 内 100% 训练准确率\n",
    "weight_scale = 1e-2   # 权重初始化标准差，可调\n",
    "learning_rate = 1e-4  # SGD 学习率，可调\n",
    "\n",
    "# 构造三层网络（两个 100 神经元隐藏层）\n",
    "model = FullyConnectedNet(\n",
    "    [100, 100],\n",
    "    weight_scale=weight_scale,\n",
    "    dtype=np.float64\n",
    ")\n",
    "\n",
    "# 创建通用训练器 Solver\n",
    "solver = Solver(\n",
    "    model,\n",
    "    small_data,\n",
    "    print_every=10,                # 每 10 次迭代打印一次\n",
    "    num_epochs=20,                 # 总共训练 20 个 epoch\n",
    "    batch_size=25,                 # 每个 mini-batch 25 张图\n",
    "    update_rule=\"sgd\",             # 使用普通 SGD\n",
    "    optim_config={\"learning_rate\": learning_rate},\n",
    ")\n",
    "\n",
    "solver.train()  # 开始训练\n",
    "\n",
    "# 绘制训练损失曲线，观察是否迅速降到接近 0，验证过拟合\n",
    "plt.plot(solver.loss_history)\n",
    "plt.title(\"训练损失历史\")\n",
    "plt.xlabel(\"迭代次数\")\n",
    "plt.ylabel(\"训练损失\")\n",
    "plt.grid(linestyle='--', linewidth=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJHKHQUIu-pX"
   },
   "source": [
    "接下来，请尝试使用一个 **五层网络**，每层 100 个神经元，在 50 张训练样本上实现过拟合。  \n",
    "你依旧需要手动调整 **学习率** 和 **权重初始化尺度**，  \n",
    "但同样应当在 **20 个 epoch 内** 达到 **100% 的训练准确率**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vbp8Nsayu-pX"
   },
   "outputs": [],
   "source": [
    "# TODO：仅用五层网络（每层 100 个神经元）在 50 个训练样本上过拟合，\n",
    "# 只需调节学习率和权重初始化尺度即可。\n",
    "\n",
    "num_train = 50  # 仅取前 50 张图片作为极小训练集\n",
    "small_data = {\n",
    "    'X_train': data['X_train'][:num_train],\n",
    "    'y_train': data['y_train'][:num_train],\n",
    "    'X_val':   data['X_val'],\n",
    "    'y_val':   data['y_val'],\n",
    "}\n",
    "\n",
    "# 下面两个超参数需要你手动尝试不同数值，以便在 20 个 epoch 内达到 100% 训练准确率\n",
    "learning_rate = 2e-3  # 学习率，可调\n",
    "weight_scale  = 1e-5  # 权重初始化标准差，可调\n",
    "\n",
    "# 构造五层网络（四个 100 神经元隐藏层）\n",
    "model = FullyConnectedNet(\n",
    "    [100, 100, 100, 100],\n",
    "    weight_scale=weight_scale,\n",
    "    dtype=np.float64\n",
    ")\n",
    "\n",
    "# 创建通用训练器 Solver\n",
    "solver = Solver(\n",
    "    model,\n",
    "    small_data,\n",
    "    print_every=10,               # 每 10 次迭代打印一次\n",
    "    num_epochs=20,                # 总共训练 20 个 epoch\n",
    "    batch_size=25,                # 每个 mini-batch 25 张图\n",
    "    update_rule='sgd',            # 使用普通 SGD\n",
    "    optim_config={'learning_rate': learning_rate},\n",
    ")\n",
    "\n",
    "solver.train()  # 开始训练\n",
    "\n",
    "# 绘制训练损失曲线，观察是否迅速降到接近 0，验证过拟合\n",
    "plt.plot(solver.loss_history)\n",
    "plt.title('训练损失历史')\n",
    "plt.xlabel('迭代次数')\n",
    "plt.ylabel('训练损失')\n",
    "plt.grid(linestyle='--', linewidth=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOdiuVmCu-pX",
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "## 内联问题 1：\n",
    "在训练三层网络和五层网络时，你有没有发现什么差异？  \n",
    "特别是，根据你的实际操作，哪一个网络对**初始化尺度**更敏感？你觉得为什么会这样？\n",
    "\n",
    "## 答案：\n",
    "[请在此处填写]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uWe_zEgFu-pY"
   },
   "source": [
    "# 参数更新规则（Update Rules）\n",
    "\n",
    "迄今为止，我们一直使用最普通的随机梯度下降（vanilla SGD）作为参数更新规则。  \n",
    "更先进的更新规则能够显著降低深度网络的训练难度。  \n",
    "接下来我们将实现几种最常用的更新规则，并将它们与 vanilla SGD 进行对比。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9l25b5jAu-pY"
   },
   "source": [
    "## SGD + Momentum\n",
    "带动量的随机梯度下降（SGD+Momentum）是一种广泛使用的参数更新规则，相比最原始的随机梯度下降，它通常能让深度网络收敛得更快。  \n",
    "更多细节可参考 Momentum Update 章节：http://cs231n.github.io/neural-networks-3/#sgd\n",
    "\n",
    "打开文件 `cs231n/optim.py`，先阅读文件顶部的 API 文档，确保你理解接口要求。  \n",
    "然后在 `sgd_momentum` 函数中实现 **SGD+Momentum** 更新规则，并运行下方代码检查实现是否正确。  \n",
    "检查时应看到所有相对误差小于 1e-8。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fR5m7nuXu-pY"
   },
   "outputs": [],
   "source": [
    "# 从 cs231n.optim 中导入 sgd_momentum 函数，用于测试你的实现\n",
    "from cs231n.optim import sgd_momentum\n",
    "\n",
    "# 构造测试数据：权重 w、梯度 dw、动量 v\n",
    "N, D = 4, 5\n",
    "w  = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "v  = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "# 配置字典：学习率 + 当前动量\n",
    "config = {\"learning_rate\": 1e-3, \"velocity\": v}\n",
    "\n",
    "# 调用你实现的 sgd_momentum，得到下一次权重 next_w 与更新后的动量\n",
    "next_w, _ = sgd_momentum(w, dw, config=config)\n",
    "\n",
    "# 期望的正确结果（由官方给出的参考实现计算）\n",
    "expected_next_w = np.asarray([\n",
    "    [0.1406,      0.20738947,  0.27417895,  0.34096842,  0.40775789],\n",
    "    [0.47454737,  0.54133684,  0.60812632,  0.67491579,  0.74170526],\n",
    "    [0.80849474,  0.87528421,  0.94207368,  1.00886316,  1.07565263],\n",
    "    [1.14244211,  1.20923158,  1.27602105,  1.34281053,  1.4096    ]])\n",
    "expected_velocity = np.asarray([\n",
    "    [0.5406,      0.55475789,  0.56891579, 0.58307368,  0.59723158],\n",
    "    [0.61138947,  0.62554737,  0.63970526,  0.65386316,  0.66802105],\n",
    "    [0.68217895,  0.69633684,  0.71049474,  0.72465263,  0.73881053],\n",
    "    [0.75296842,  0.76712632,  0.78128421,  0.79544211,  0.8096    ]])\n",
    "\n",
    "# 计算并打印相对误差，应接近 1e-8 或更小\n",
    "print(\"next_w 相对误差: \", rel_error(next_w, expected_next_w))\n",
    "print(\"velocity 相对误差: \", rel_error(expected_velocity, config[\"velocity\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3A5WZm78u-pY"
   },
   "source": [
    "完成上述步骤后，运行下面的代码，用 **普通 SGD** 和 **SGD + Momentum** 分别训练一个六层网络。  \n",
    "你应该能看到采用 **SGD + Momentum** 更新规则时，网络收敛得更快。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z66zjXhCu-pZ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_train = 4000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "solvers = {}\n",
    "\n",
    "for update_rule in ['sgd', 'sgd_momentum']:\n",
    "    print('Running with ', update_rule)\n",
    "    model = FullyConnectedNet(\n",
    "        [100, 100, 100, 100, 100],\n",
    "        weight_scale=5e-2\n",
    "    )\n",
    "\n",
    "    solver = Solver(\n",
    "        model,\n",
    "        small_data,\n",
    "        num_epochs=5,\n",
    "        batch_size=100,\n",
    "        update_rule=update_rule,\n",
    "        optim_config={'learning_rate': 5e-3},\n",
    "        verbose=True,\n",
    "    )\n",
    "    solvers[update_rule] = solver\n",
    "    solver.train()\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 15))\n",
    "\n",
    "axes[0].set_title('Training loss')\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[1].set_title('Training accuracy')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[2].set_title('Validation accuracy')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "\n",
    "for update_rule, solver in solvers.items():\n",
    "    axes[0].plot(solver.loss_history, label=f\"loss_{update_rule}\")\n",
    "    axes[1].plot(solver.train_acc_history, label=f\"train_acc_{update_rule}\")\n",
    "    axes[2].plot(solver.val_acc_history, label=f\"val_acc_{update_rule}\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.legend(loc=\"best\", ncol=4)\n",
    "    ax.grid(linestyle='--', linewidth=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0EQBM5xu-pZ"
   },
   "source": [
    "## RMSProp 与 Adam  \n",
    "RMSProp [1] 和 Adam [2] 是两种更新规则，它们通过维护梯度的**二阶矩**（即平方梯度）的滑动平均，来为每个参数单独设置学习率。\n",
    "\n",
    "请在文件 `cs231n/optim.py` 中完成以下实现：  \n",
    "- 在 `rmsprop` 函数里实现 **RMSProp** 更新规则；  \n",
    "- 在 `adam` 函数里实现 **Adam** 更新规则。  \n",
    "\n",
    "**注意：** 请实现**完整的 Adam** 更新规则（包含偏差修正机制），而不是课程笔记中最初提到的简化版本。\n",
    "\n",
    "[1] Tijmen Tieleman 和 Geoffrey Hinton. “Lecture 6.5-RMSProp: Divide the gradient by a running average of its recent magnitude.” COURSERA: Neural Networks for Machine Learning 4 (2012).  \n",
    "[2] Diederik Kingma 和 Jimmy Ba, “Adam: A Method for Stochastic Optimization”, ICLR 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1w_hTsl-u-pZ"
   },
   "outputs": [],
   "source": [
    "# 测试 RMSProp 实现\n",
    "from cs231n.optim import rmsprop\n",
    "\n",
    "# 构造测试数据\n",
    "N, D = 4, 5\n",
    "w     = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)   # 当前权重\n",
    "dw    = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)   # 当前梯度\n",
    "cache = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)    # 缓存的平方梯度滑动平均\n",
    "\n",
    "# 配置字典：学习率 + 当前 cache\n",
    "config = {'learning_rate': 1e-2, 'cache': cache}\n",
    "next_w, _ = rmsprop(w, dw, config=config)\n",
    "\n",
    "# 官方给出的期望结果\n",
    "expected_next_w = np.asarray([\n",
    "  [-0.39223849, -0.34037513, -0.28849239, -0.23659121, -0.18467247],\n",
    "  [-0.132737,   -0.08078555, -0.02881884,  0.02316247,  0.07515774],\n",
    "  [ 0.12716641,  0.17918792,  0.23122175,  0.28326742,  0.33532447],\n",
    "  [ 0.38739248,  0.43947102,  0.49155973,  0.54365823,  0.59576619]])\n",
    "expected_cache = np.asarray([\n",
    "  [ 0.5976,      0.6126277,   0.6277108,   0.64284931,  0.65804321],\n",
    "  [ 0.67329252,  0.68859723,  0.70395734,  0.71937285,  0.73484377],\n",
    "  [ 0.75037008,  0.7659518,   0.78158892,  0.79728144,  0.81302936],\n",
    "  [ 0.82883269,  0.84469141,  0.86060554,  0.87657507,  0.8926    ]])\n",
    "\n",
    "# 计算并打印相对误差，应接近 1e-7 或更小\n",
    "print('next_w 误差: ', rel_error(expected_next_w, next_w))\n",
    "print('cache 误差: ', rel_error(expected_cache, config['cache']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s96oB_Lmu-pa"
   },
   "outputs": [],
   "source": [
    "# 测试 Adam 实现\n",
    "from cs231n.optim import adam\n",
    "\n",
    "# 构造测试数据\n",
    "N, D = 4, 5\n",
    "w  = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)   # 当前权重\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)   # 当前梯度\n",
    "m  = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)    # 一阶动量（梯度均值）\n",
    "v  = np.linspace(0.7, 0.5, num=N*D).reshape(N, D)    # 二阶动量（梯度平方均值）\n",
    "\n",
    "# 配置字典：学习率 + 当前一阶/二阶动量 + 迭代计数 t\n",
    "config = {'learning_rate': 1e-2, 'm': m, 'v': v, 't': 5}\n",
    "next_w, _ = adam(w, dw, config=config)\n",
    "\n",
    "# 官方给出的期望结果（含完整的偏差修正）\n",
    "expected_next_w = np.asarray([\n",
    "    [-0.40094747, -0.34836187, -0.29577703, -0.24319299, -0.19060977],\n",
    "    [-0.1380274,  -0.08544591, -0.03286534,  0.01971428,  0.0722929],\n",
    "    [ 0.1248705,   0.17744702,  0.23002243,  0.28259667,  0.33516969],\n",
    "    [ 0.38774145,  0.44031188,  0.49288093,  0.54544852,  0.59801459]])\n",
    "expected_v = np.asarray([\n",
    "    [0.69966,     0.68908382,  0.67851319,  0.66794809,  0.65738853],\n",
    "    [0.64683452,  0.63628604,  0.6257431,   0.61520571,  0.60467385],\n",
    "    [0.59414753,  0.58362676,  0.57311152,  0.56260183,  0.55209767],\n",
    "    [0.54159906,  0.53110598,  0.52061845,  0.51013645,  0.49966   ]])\n",
    "expected_m = np.asarray([\n",
    "    [0.48,        0.49947368,  0.51894737,  0.53842105,  0.55789474],\n",
    "    [0.57736842,  0.59684211,  0.61631579,  0.63578947,  0.65526316],\n",
    "    [0.67473684,  0.69421053,  0.71368421,  0.73315789,  0.75263158],\n",
    "    [0.77210526,  0.79157895,  0.81105263,  0.83052632,  0.85      ]])\n",
    "\n",
    "# 计算并打印相对误差，应接近 1e-7 或更小\n",
    "print('next_w 误差: ', rel_error(expected_next_w, next_w))\n",
    "print('v 误差: ', rel_error(expected_v, config['v']))\n",
    "print('m 误差: ', rel_error(expected_m, config['m']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DB7lZjH9u-pa"
   },
   "source": [
    "当你已经调试好 RMSProp 和 Adam 的实现后，请运行下面的代码，用这两种新的更新规则分别训练一组深层网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Otf_c60Qu-pa",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 为 RMSProp 与 Adam 分别设置不同的学习率\n",
    "learning_rates = {'rmsprop': 1e-4, 'adam': 1e-3}\n",
    "\n",
    "# 依次用 'adam' 和 'rmsprop' 两种更新规则训练网络\n",
    "for update_rule in ['adam', 'rmsprop']:\n",
    "    print('开始运行：', update_rule)\n",
    "\n",
    "    # 构建五层全连接网络（每层 100 个神经元）\n",
    "    model = FullyConnectedNet(\n",
    "        [100, 100, 100, 100, 100],\n",
    "        weight_scale=5e-2\n",
    "    )\n",
    "\n",
    "    # 使用通用训练器 Solver\n",
    "    solver = Solver(\n",
    "        model,\n",
    "        small_data,\n",
    "        num_epochs=5,                              # 训练 5 个 epoch\n",
    "        batch_size=100,                            # 每个 mini-batch 100 张图\n",
    "        update_rule=update_rule,                   # 指定优化器\n",
    "        optim_config={'learning_rate': learning_rates[update_rule]},  # 对应学习率\n",
    "        verbose=True                               # 打印训练日志\n",
    "    )\n",
    "    solvers[update_rule] = solver\n",
    "    solver.train()                               # 开始训练\n",
    "    print()                                      # 空行方便阅读\n",
    "\n",
    "# 绘制训练过程中的损失、训练准确率、验证准确率曲线\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 15))\n",
    "\n",
    "axes[0].set_title('训练损失')\n",
    "axes[0].set_xlabel('迭代次数')\n",
    "\n",
    "axes[1].set_title('训练准确率')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "\n",
    "axes[2].set_title('验证准确率')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "\n",
    "for update_rule, solver in solvers.items():\n",
    "    axes[0].plot(solver.loss_history,      label=f\"{update_rule}\")\n",
    "    axes[1].plot(solver.train_acc_history, label=f\"{update_rule}\")\n",
    "    axes[2].plot(solver.val_acc_history,   label=f\"{update_rule}\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.legend(loc='best', ncol=4)           # 图例\n",
    "    ax.grid(linestyle='--', linewidth=0.5)  # 网格线便于观察趋势\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTSO9l58u-pa",
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "## 内联问题 2：\n",
    "\n",
    "AdaGrad 与 Adam 类似，是一种**逐参数**的优化方法，其更新规则如下：\n",
    "\n",
    "```\n",
    "cache += dw**2\n",
    "w += - learning_rate * dw / (np.sqrt(cache) + eps)\n",
    "```\n",
    "\n",
    "John 发现，当使用 AdaGrad 训练网络时，更新量变得越来越小，网络学习速度变慢。  \n",
    "请结合 AdaGrad 的更新规则，解释为什么会出现这种现象？Adam 是否也会遇到同样的问题？\n",
    "\n",
    "## 答案：\n",
    "[请在此处填写]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUktPczWu-pa"
   },
   "source": [
    "# 训练一个优秀的模型！  \n",
    "请你在 CIFAR-10 上训练一个性能最好的全连接网络，并将最优模型保存到变量 `best_model` 中。  \n",
    "我们要求你的模型在验证集上至少达到 **50% 的准确率**。\n",
    "\n",
    "如果你认真调参，完全有可能把验证准确率提高到 **55% 以上**，但这部分不做强制要求，也不会额外加分。  \n",
    "在下一项作业中，我们会要求你训练一个性能最好的卷积神经网络；因此我们更希望你把主要精力放在卷积网络上，而非全连接网络。\n",
    "\n",
    "**提示：** 在下一项作业里，你将学到 **批归一化（BatchNormalization）** 和 **Dropout** 等技术，它们能帮助你训练更强大的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xLiabOb3u-pb",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_model = None\n",
    "\n",
    "################################################################################\n",
    "# 代办：在 CIFAR-10 上训练你能得到的最佳 FullyConnectedNet。你可以尝试使用      #\n",
    "# 批归一化（batch normalization）、层归一化（layer normalization）或 Dropout。  #\n",
    "# 请将表现最好的模型保存到变量 best_model 中。                                  #\n",
    "################################################################################\n",
    "# *****你的代码开始（请勿删除或修改此行）*****\n",
    "\n",
    "\n",
    "\n",
    "# *****你的代码结束（请勿删除或修改此行）*****\n",
    "################################################################################\n",
    "#                              代码结束                                        #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Dw4p7kIu-pb"
   },
   "source": [
    "# 测试你的模型！  \n",
    "用你训练出的最优模型在验证集和测试集上运行。  \n",
    "你应当在**验证集**和**测试集**上都至少达到 **50% 的准确率**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-mWD7-Su-pb",
    "test": "val_test_accuracy"
   },
   "outputs": [],
   "source": [
    "# 使用最优模型在测试集和验证集上预测类别\n",
    "y_test_pred = np.argmax(best_model.loss(data['X_test']), axis=1)  # 测试集预测\n",
    "y_val_pred  = np.argmax(best_model.loss(data['X_val']), axis=1)   # 验证集预测\n",
    "\n",
    "# 计算并打印准确率\n",
    "print('验证集准确率: ', (y_val_pred == data['y_val']).mean())\n",
    "print('测试集准确率: ', (y_test_pred == data['y_test']).mean())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
