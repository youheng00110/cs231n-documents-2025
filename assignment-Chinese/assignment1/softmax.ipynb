{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a1c548",
   "metadata": {
    "id": "a7a1c548"
   },
   "outputs": [],
   "source": [
    "# 将 Google Drive 挂载到 Colab 虚拟机，使其像本地文件夹一样可用\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# TODO：请填写你在 Google Drive 中存放解压后作业文件夹的路径\n",
    "# 示例：'cs231n/assignments/assignment1/'\n",
    "# 请根据你的实际路径进行修改\n",
    "FOLDERNAME = 'cs231n/assignments/assignment1/'\n",
    "assert FOLDERNAME is not None, \"[!] 请填写正确的文件夹路径。\"\n",
    "\n",
    "# 挂载完成后，把该路径加入 Python 的模块搜索路径，\n",
    "# 确保 Colab 虚拟机的解释器能找到并加载其中的 .py 文件\n",
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
    "\n",
    "# 如果 CIFAR-10 数据集尚未存在，则将其下载到 Google Drive\n",
    "# 进入数据集目录\n",
    "%cd /content/drive/My\\ Drive/$FOLDERNAME/cs231n/datasets/\n",
    "# 执行下载脚本，自动下载并解压 CIFAR-10\n",
    "!bash get_datasets.sh\n",
    "# 下载完成后返回作业根目录\n",
    "%cd /content/drive/My\\ Drive/$FOLDERNAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08abd1b",
   "metadata": {
    "id": "f08abd1b",
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Softmax 分类器练习\n",
    "\n",
    "*请将这份完整的练习表（包括所有输出结果以及表格外部的支持代码）随本次作业一并提交。更多详情见课程官网的[作业页面](http://vision.stanford.edu/teaching/cs231n/assignments.html)。*\n",
    "\n",
    "在本练习中，你将：\n",
    "\n",
    "- 实现 Softmax 分类器的**完全向量化损失函数**。\n",
    "- 实现其**解析梯度**的完全向量化表达式。\n",
    "- 使用数值梯度**验证你的实现**。\n",
    "- 使用验证集**调整学习率和正则化强度**。\n",
    "- 使用**随机梯度下降（SGD）**优化损失函数。\n",
    "- **可视化**最终学习到的权重。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3defe387",
   "metadata": {
    "id": "3defe387",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# 运行本 notebook 所需的初始化代码\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10   # 加载 CIFAR-10 数据的工具函数\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 让 matplotlib 图形直接嵌入在 notebook 中，而不是弹出新窗口\n",
    "%matplotlib inline\n",
    "# 统一设置图片默认大小，便于查看\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "# 关闭插值，使像素边界更清晰\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "# 灰度图默认使用灰度色图\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# 开启 IPython 的自动重载功能：修改外部 .py 文件后无需重启 kernel 即可生效\n",
    "# 官方讨论贴地址：http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "# 注意：由于网络原因，上述链接暂时无法成功解析。如果你需要查看该网页内容，\n",
    "# 请检查链接是否有效，或稍后重试；若不需要查看，可直接继续后续操作。\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8603125",
   "metadata": {
    "id": "b8603125",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "## CIFAR-10 数据加载与预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bf85b0",
   "metadata": {
    "id": "23bf85b0",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# 加载原始 CIFAR-10 数据\n",
    "cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "\n",
    "# 如果之前已加载过数据，先删除相关变量，防止重复加载导致内存占用过高\n",
    "try:\n",
    "    del X_train, y_train\n",
    "    del X_test, y_test\n",
    "    print('已清空之前加载的数据。')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# 真正从磁盘读取 CIFAR-10\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "# 简单检查：打印训练集和测试集的形状\n",
    "print('训练数据形状: ', X_train.shape)\n",
    "print('训练标签形状: ', y_train.shape)\n",
    "print('测试数据形状: ', X_test.shape)\n",
    "print('测试标签形状: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471ea325",
   "metadata": {
    "id": "471ea325",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# 可视化数据集中的一些样本\n",
    "# 从每个类别中随机展示若干张训练图像\n",
    "\n",
    "classes = ['飞机', '汽车', '鸟类', '猫', '鹿', '狗', '青蛙', '马', '船', '卡车']\n",
    "num_classes = len(classes)              # 类别数（10）\n",
    "samples_per_class = 7                   # 每类展示 7 张图\n",
    "\n",
    "for y, cls in enumerate(classes):\n",
    "    # 找到所有属于当前类别的训练样本索引\n",
    "    idxs = np.flatnonzero(y_train == y)\n",
    "    # 随机选择 7 张且不重复\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    \n",
    "    for i, idx in enumerate(idxs):\n",
    "        # 计算子图位置\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        # 显示图像\n",
    "        plt.imshow(X_train[idx].astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        # 只在第一行写类别名\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd1f984",
   "metadata": {
    "id": "9bd1f984",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# 将数据划分为训练集、验证集和测试集。此外，我们还将从训练集中创建一个小型开发集，\n",
    "# 作为训练集的一个子集；我们可以在开发过程中使用这个开发集，这样代码运行会更快。\n",
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "num_test = 1000\n",
    "num_dev = 500\n",
    "\n",
    "# 验证集是从原始训练集中选取的 num_validation 个样本\n",
    "mask = range(num_training, num_training + num_validation)\n",
    "X_val = X_train[mask]\n",
    "y_val = y_train[mask]\n",
    "\n",
    "# 训练集是从原始训练集中选取的前 num_training 个样本\n",
    "mask = range(num_training)\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "# 开发集是从训练集中随机选取的 num_dev 个样本\n",
    "mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "X_dev = X_train[mask]\n",
    "y_dev = y_train[mask]\n",
    "\n",
    "# 测试集是从原始测试集中选取的前 num_test 个样本\n",
    "mask = range(num_test)\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "\n",
    "# 打印各数据集的形状\n",
    "print('训练数据形状: ', X_train.shape)\n",
    "print('训练标签形状: ', y_train.shape)\n",
    "print('验证数据形状: ', X_val.shape)\n",
    "print('验证标签形状: ', y_val.shape)\n",
    "print('测试数据形状: ', X_test.shape)\n",
    "print('测试标签形状: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdd02f9",
   "metadata": {
    "id": "9bdd02f9",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# 预处理：将图像数据展平成行向量\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))  # 展平训练集\n",
    "X_val   = np.reshape(X_val,   (X_val.shape[0],   -1))  # 展平验证集\n",
    "X_test  = np.reshape(X_test,  (X_test.shape[0],  -1))  # 展平测试集\n",
    "X_dev   = np.reshape(X_dev,   (X_dev.shape[0],   -1))  # 展平开发集\n",
    "\n",
    "# 简单检查：打印各数据集的形状\n",
    "print('训练数据形状: ', X_train.shape)\n",
    "print('验证数据形状: ', X_val.shape)\n",
    "print('测试数据形状: ', X_test.shape)\n",
    "print('开发数据形状: ', X_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066fffc7",
   "metadata": {
    "id": "066fffc7",
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [],
   "source": [
    "# 预处理：减去均值图像\n",
    "# 首先：根据训练数据计算均值图像\n",
    "mean_image = np.mean(X_train, axis=0)  # 按列计算均值\n",
    "print(mean_image[:10])  # 打印前 10 个元素\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(mean_image.reshape((32, 32, 3)).astype('uint8'))  # 可视化均值图像\n",
    "plt.show()\n",
    "\n",
    "# 其次：从训练数据和测试数据中减去均值图像\n",
    "X_train -= mean_image\n",
    "X_val -= mean_image\n",
    "X_test -= mean_image\n",
    "X_dev -= mean_image\n",
    "\n",
    "# 最后：添加偏置维度（全为 1 的列），即偏置技巧，这样分类器只需优化一个权重矩阵 W。\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "\n",
    "# 打印各数据集的形状\n",
    "print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605c22f8",
   "metadata": {
    "id": "605c22f8"
   },
   "source": [
    "## Softmax 分类器\n",
    "\n",
    "本节的所有代码都将写在 `cs231n/classifiers/softmax.py` 文件中。\n",
    "\n",
    "正如你所看到的，我们已经预先填充了函数 `softmax_loss_naive`，它使用 for 循环来计算 Softmax 损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56073f14",
   "metadata": {
    "id": "56073f14"
   },
   "outputs": [],
   "source": [
    "# 测试我们提供的朴素 Softmax 损失函数实现\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# 生成一个随机的 Softmax 分类器权重矩阵，数值很小\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "\n",
    "# 计算损失和梯度\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "print('损失: %f' % loss)\n",
    "\n",
    "# 简单检查：损失值应接近 -log(0.1)\n",
    "print('损失: %f' % loss)\n",
    "print('合理性检查: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NT_PDkFhZlMy",
   "metadata": {
    "id": "NT_PDkFhZlMy"
   },
   "source": [
    "**内联问题 1**\n",
    "\n",
    "为什么我们期望损失接近 -log(0.1)？请简要解释。\n",
    "\n",
    "$\\color{blue}{\\textit{你的答案：}}$ *请在此处填写*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dad4937",
   "metadata": {
    "id": "7dad4937"
   },
   "source": [
    "目前，上述函数返回的 `grad` 全部为零。  \n",
    "请推导并实现 softmax 损失函数的梯度，并在 `softmax_loss_naive` 函数内部直接完成实现。  \n",
    "建议将新代码穿插在现有函数中，而非重写整个函数。\n",
    "\n",
    "为了验证你的梯度实现是否正确，可以利用数值方法估计损失函数的梯度，并将其与你计算出的解析梯度进行比较。  \n",
    "我们已为你提供了相应的检查代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d5ad22",
   "metadata": {
    "id": "f7d5ad22"
   },
   "outputs": [],
   "source": [
    "# 一旦你实现了梯度，请用下面的代码重新计算\n",
    "# 并用我们提供的函数进行梯度检查\n",
    "\n",
    "# 计算当前权重 W 下的损失及梯度（无正则化）\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# 在若干随机维度上数值计算梯度，并与解析梯度对比；\n",
    "# 两者应在所有维度上几乎完全一致。\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad)\n",
    "\n",
    "# 再次进行梯度检查，这次开启正则化；\n",
    "# 别忘了正则项的梯度也要一起实现！\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665fc684",
   "metadata": {
    "id": "665fc684",
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**内联问题 2**\n",
    "\n",
    "尽管 gradcheck 对于 Softmax 损失通常是可靠的，但在 SVM 损失上偶尔会出现某个维度与数值梯度不完全匹配的情况。  \n",
    "这种差异可能由什么原因引起？是否需要担心？  \n",
    "请举一个最简单的一维例子说明 SVM 损失梯度检查可能失败的情形。  \n",
    "改变 margin Δ 会如何影响这种现象的出现频率？\n",
    "\n",
    "提示：SVM 损失函数在严格意义上并不可微。\n",
    "\n",
    "$\\color{blue}{\\textit{你的答案：}}$ *请在此处填写*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310ca148",
   "metadata": {
    "id": "310ca148",
    "scrolled": true,
    "test": "vectorized_time_1"
   },
   "outputs": [],
   "source": [
    "# 接下来实现 softmax_loss_vectorized 函数；目前只需计算损失值，\n",
    "# 稍后再实现梯度。\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('朴素实现损失: %e，耗时 %f 秒' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, _ = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('向量化实现损失: %e，耗时 %f 秒' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# 两种实现计算出的损失值应一致，但向量化版本应快得多。\n",
    "print('损失差值: %f' % (loss_naive - loss_vectorized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5d3643",
   "metadata": {
    "id": "ef5d3643",
    "test": "vectorized_time_2"
   },
   "outputs": [],
   "source": [
    "# 补全 softmax_loss_vectorized 的实现，并以向量化方式计算损失函数的梯度。\n",
    "\n",
    "# 朴素实现与向量化实现应得到相同结果，但向量化版本速度应更快。\n",
    "tic = time.time()\n",
    "_, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('朴素实现损失与梯度：耗时 %f 秒' % (toc - tic))\n",
    "\n",
    "tic = time.time()\n",
    "_, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('向量化实现损失与梯度：耗时 %f 秒' % (toc - tic))\n",
    "\n",
    "# 损失是一个标量，直接比较即可；梯度是矩阵，用 Frobenius 范数比较。\n",
    "difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('梯度差异: %f' % difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635eef89",
   "metadata": {
    "id": "635eef89"
   },
   "source": [
    "### 随机梯度下降（SGD）\n",
    "\n",
    "我们现在已经得到了向量化且高效计算损失、梯度的实现，并且梯度与数值梯度一致，因此可以使用随机梯度下降（SGD）来最小化损失。  \n",
    "本部分的代码将写在 `cs231n/classifiers/linear_classifier.py` 中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a13952",
   "metadata": {
    "id": "a5a13952",
    "test": "sgd"
   },
   "outputs": [],
   "source": [
    "# 在文件 linear_classifier.py 中，请在函数 LinearClassifier.train() 里实现 SGD，\n",
    "# 然后用以下代码运行它。\n",
    "\n",
    "from cs231n.classifiers import Softmax\n",
    "softmax = Softmax()                 # 创建 Softmax 分类器实例\n",
    "\n",
    "tic = time.time()\n",
    "# 开始训练，使用给定的学习率、正则化系数，迭代 1500 次，并打印日志\n",
    "loss_hist = softmax.train(\n",
    "    X_train, y_train,\n",
    "    learning_rate=1e-7,\n",
    "    reg=2.5e4,\n",
    "    num_iters=1500,\n",
    "    verbose=True\n",
    ")\n",
    "toc = time.time()\n",
    "print('训练耗时 %f 秒' % (toc - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abad0c45",
   "metadata": {
    "id": "abad0c45"
   },
   "outputs": [],
   "source": [
    "# 一种实用的调试策略是把损失值随迭代次数画出来\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel('迭代次数')\n",
    "plt.ylabel('损失值')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e54af4f",
   "metadata": {
    "id": "2e54af4f",
    "test": "validate"
   },
   "outputs": [],
   "source": [
    "# 实现 LinearClassifier.predict 函数，并评估在训练集和验证集上的表现\n",
    "# 你应在验证集上获得约 0.34 (> 0.33) 的准确率\n",
    "\n",
    "# 在训练集上预测\n",
    "y_train_pred = softmax.predict(X_train)\n",
    "print('训练准确率: %f' % (np.mean(y_train == y_train_pred), ))\n",
    "\n",
    "# 在验证集上预测\n",
    "y_val_pred = softmax.predict(X_val)\n",
    "print('验证准确率: %f' % (np.mean(y_val == y_val_pred), ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O4ac8brXSxeo",
   "metadata": {
    "id": "O4ac8brXSxeo"
   },
   "outputs": [],
   "source": [
    "# 将训练好的模型保存，以便自动评分器使用\n",
    "softmax.save(\"softmax.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce2da33",
   "metadata": {
    "id": "1ce2da33",
    "tags": [
     "code"
    ],
    "test": "tuning"
   },
   "outputs": [],
   "source": [
    "# 使用验证集来调优超参数（正则化强度和学习率）。\n",
    "# 建议尝试不同的学习率和正则化强度范围；认真调参后，\n",
    "# 你应在验证集上得到约 0.365（> 0.36）的准确率。\n",
    "#\n",
    "# 注意：在超参数搜索过程中可能会出现运行时/溢出警告，\n",
    "# 这通常是由极端值引起的，并不是 bug。\n",
    "\n",
    "# results 是一个字典，以 (learning_rate, regularization_strength) 为键，\n",
    "# 对应的值为 (training_accuracy, validation_accuracy)。\n",
    "# 准确率定义为被正确分类的数据点所占的比例。\n",
    "results = {}\n",
    "best_val = -1        # 目前观察到的最高验证准确率\n",
    "best_softmax = None  # 达到最高验证准确率的 Softmax 对象\n",
    "\n",
    "################################################################################\n",
    "# 代办:                                                                         #\n",
    "# 编写代码，在验证集上调优超参数。对于每一组超参数组合，                           #\n",
    "# 在训练集上训练一个 Softmax 分类器，计算其在训练集和验证集上的准确率，            #\n",
    "# 并将结果存入 results 字典。此外，将最高验证准确率存入 best_val，                #\n",
    "# 并将达到该准确率的 Softmax 对象存入 best_softmax。                             #\n",
    "#                                                                              #\n",
    "# 提示：开发验证代码时可使用较小的 num_iters，这样训练更快；                      #\n",
    "# 确认代码正确后，再用更大的 num_iters 重新运行。                                #\n",
    "################################################################################\n",
    "\n",
    "# 以下超参数仅供参考，你可以根据需要自行调整\n",
    "learning_rates = [1e-7, 1e-6]\n",
    "regularization_strengths = [2.5e4, 1e4]\n",
    "\n",
    "# 打印最终结果\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e 训练准确率: %f 验证准确率: %f' %\n",
    "          (lr, reg, train_accuracy, val_accuracy))\n",
    "\n",
    "print('交叉验证期间达到的最佳验证准确率: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b520723",
   "metadata": {
    "id": "1b520723",
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [],
   "source": [
    "# 可视化交叉验证结果\n",
    "import math\n",
    "import pdb\n",
    "\n",
    "# pdb.set_trace()\n",
    "\n",
    "# 将学习率和正则化强度取对数，方便作图\n",
    "x_scatter = [math.log10(x[0]) for x in results]   # log10(learning_rate)\n",
    "y_scatter = [math.log10(x[1]) for x in results]   # log10(regularization_strength)\n",
    "\n",
    "marker_size = 100\n",
    "\n",
    "# 绘制训练准确率热力图\n",
    "colors = [results[x][0] for x in results]                # 训练准确率\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.tight_layout(pad=3)\n",
    "plt.scatter(x_scatter, y_scatter, marker_size, c=colors, cmap=plt.cm.coolwarm)\n",
    "plt.colorbar()\n",
    "plt.xlabel('log10(学习率)')\n",
    "plt.ylabel('log10(正则化强度)')\n",
    "plt.title('CIFAR-10 训练准确率')\n",
    "\n",
    "# 绘制验证准确率热力图\n",
    "colors = [results[x][1] for x in results]                # 验证准确率\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.scatter(x_scatter, y_scatter, marker_size, c=colors, cmap=plt.cm.coolwarm)\n",
    "plt.colorbar()\n",
    "plt.xlabel('log10(学习率)')\n",
    "plt.ylabel('log10(正则化强度)')\n",
    "plt.title('CIFAR-10 验证准确率')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b29768e",
   "metadata": {
    "id": "7b29768e",
    "test": "test"
   },
   "outputs": [],
   "source": [
    "# 在测试集上评估最优 Softmax 模型\n",
    "y_test_pred = best_softmax.predict(X_test)               # 预测测试集标签\n",
    "test_accuracy = np.mean(y_test == y_test_pred)           # 计算测试准确率\n",
    "print('基于原始像素的 Softmax 分类器最终测试集准确率: %f' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JU4xmsWpUIAd",
   "metadata": {
    "id": "JU4xmsWpUIAd"
   },
   "outputs": [],
   "source": [
    "# 保存最优的 Softmax 模型\n",
    "best_softmax.save(\"best_softmax.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e122c35",
   "metadata": {
    "id": "9e122c35",
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [],
   "source": [
    "# 可视化每个类别学习到的权重（模板）\n",
    "# 权重的美观程度取决于你选择的学习率和正则化强度\n",
    "w = best_softmax.W[:-1, :]           # 去掉偏置项\n",
    "w = w.reshape(32, 32, 3, 10)         # 重新调整形状为 (32,32,3,10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)  # 找到最小、最大值用于归一化\n",
    "classes = ['飞机', '汽车', '鸟类', '猫', '鹿',\n",
    "           '狗', '青蛙', '马', '船', '卡车']\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "\n",
    "    # 将权重线性缩放到 0–255 之间，便于显示\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b827ad",
   "metadata": {
    "id": "44b827ad",
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**内联问题 3**\n",
    "\n",
    "请描述你可视化的 Softmax 分类器权重看起来像什么，并简要解释为什么它们看起来是这样的。\n",
    "\n",
    "$\\color{blue}{\\textit{你的答案：}}$ *请在此处填写*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DvBKlibgaiy9",
   "metadata": {
    "id": "DvBKlibgaiy9"
   },
   "source": [
    "**内联问题 4** - **判断题**\n",
    "\n",
    "假设整体训练损失定义为所有训练样本的每个样本损失之和。有可能添加一个新数据点到训练集中，会改变 Softmax 损失，但不会改变 SVM 损失。\n",
    "\n",
    "$\\color{blue}{\\textit{你的答案：}}$\n",
    "\n",
    "\n",
    "$\\color{blue}{\\textit{你的解释：}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_gLYWFm7akSI",
   "metadata": {
    "id": "_gLYWFm7akSI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
