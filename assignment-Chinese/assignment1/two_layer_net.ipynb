{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f887c3",
   "metadata": {
    "id": "d0f887c3"
   },
   "outputs": [],
   "source": [
    "# 将 Google Drive 挂载到 Colab 虚拟机，使其像本地文件夹一样可用\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 代办：请填写你在 Google Drive 中存放解压后作业文件夹的路径\n",
    "# 示例：'cs231n/assignments/assignment1/'\n",
    "# 请根据你的实际路径进行修改\n",
    "FOLDERNAME = 'cs231n/assignments/assignment1/'\n",
    "assert FOLDERNAME is not None, \"[!] 请填写正确的文件夹路径。\"\n",
    "\n",
    "# 挂载完成后，把该路径加入 Python 的模块搜索路径，\n",
    "# 确保 Colab 虚拟机的解释器能找到并加载其中的 .py 文件\n",
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
    "\n",
    "# 如果 CIFAR-10 数据集尚未存在，则将其下载到 Google Drive\n",
    "# 进入数据集目录\n",
    "%cd /content/drive/My\\ Drive/$FOLDERNAME/cs231n/datasets/\n",
    "# 执行下载脚本，自动下载并解压 CIFAR-10\n",
    "!bash get_datasets.sh\n",
    "# 下载完成后返回作业根目录\n",
    "%cd /content/drive/My\\ Drive/$FOLDERNAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e370a238",
   "metadata": {
    "id": "e370a238",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "```markdown\n",
    "# 全连接神经网络\n",
    "在本练习中，我们将采用模块化方法实现全连接网络。  \n",
    "对于每一层，我们将分别实现 `forward`（前向传播）和 `backward`（反向传播）函数。  \n",
    "`forward` 函数将接收输入、权重等参数，并返回输出以及一个 `cache` 对象，`cache` 用于存储反向传播时需要的数据，例如：\n",
    "\n",
    "```python\n",
    "def layer_forward(x, w):\n",
    "    \"\"\"接收输入 x 和权重 w\"\"\"\n",
    "    # 进行一些计算 ...\n",
    "    z = # ... 一个中间值\n",
    "    # 继续进行一些计算 ...\n",
    "    out = # 最终输出\n",
    "\n",
    "    cache = (x, w, z, out)  # 用于计算梯度的值\n",
    "\n",
    "    return out, cache\n",
    "```\n",
    "\n",
    "`backward` 函数将接收上游梯度和 `cache` 对象，并返回相对于输入和权重的梯度，例如：\n",
    "\n",
    "```python\n",
    "def layer_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    接收 dout（相对于输出的损失梯度）和 cache，\n",
    "    并计算相对于输入的梯度。\n",
    "    \"\"\"\n",
    "    # 解包 cache 中的值\n",
    "    x, w, z, out = cache\n",
    "\n",
    "    # 使用 cache 中的值计算梯度\n",
    "    dx = # 相对于 x 的损失梯度\n",
    "    dw = # 相对于 w 的损失梯度\n",
    "\n",
    "    return dx, dw\n",
    "```\n",
    "\n",
    "按照这种方式实现多层后，我们将能够轻松组合它们，构建不同架构的分类器。\n",
    "```\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acf2688",
   "metadata": {
    "id": "0acf2688",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# 常规初始化设置\n",
    "from __future__ import print_function  # 确保兼容 Python 2 和 3 的打印函数\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cs231n.classifiers.fc_net import *  # 全连接网络\n",
    "from cs231n.data_utils import get_CIFAR10_data  # CIFAR-10 数据加载工具\n",
    "from cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from cs231n.solver import Solver  # 通用训练器\n",
    "\n",
    "# 让 matplotlib 图形直接嵌入在 notebook 中，而不是弹出新窗口\n",
    "%matplotlib inline\n",
    "# 统一设置图片默认大小，便于查看\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "# 关闭插值，使像素边界更清晰\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "# 灰度图默认使用灰度色图\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# 开启 IPython 的自动重载功能：修改外部 .py 文件后无需重启 kernel 即可生效\n",
    "# 官方讨论贴地址：http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "# 注意：由于网络原因，上述链接暂时无法成功解析。如果你需要查看该网页内容，\n",
    "# 请检查链接是否有效，或稍后重试；若不需要查看，可直接继续后续操作。\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# 定义相对误差计算函数，用于验证梯度\n",
    "def rel_error(x, y):\n",
    "    \"\"\"返回相对误差\"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909acc7b",
   "metadata": {
    "id": "909acc7b",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# 加载已经预处理过的 CIFAR-10 数据\n",
    "data = get_CIFAR10_data()\n",
    "\n",
    "# 打印各数据集的名称和形状\n",
    "for k, v in list(data.items()):\n",
    "    print(('%s: ' % k, v.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7851b32",
   "metadata": {
    "id": "e7851b32"
   },
   "source": [
    "# 仿射层：前向传播\n",
    "打开文件 `cs231n/layers.py` 并实现 `affine_forward` 函数。\n",
    "\n",
    "完成后，运行以下代码测试你的实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9408a08c",
   "metadata": {
    "id": "9408a08c"
   },
   "outputs": [],
   "source": [
    "# 测试 affine_forward 函数\n",
    "\n",
    "# 设置测试参数\n",
    "num_inputs = 2           # 输入样本数\n",
    "input_shape = (4, 5, 6)  # 每个输入样本的形状\n",
    "output_dim = 3           # 输出维度\n",
    "\n",
    "# 计算输入和权重的总大小\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "\n",
    "# 构造测试输入、权重和偏置\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "\n",
    "# 调用 affine_forward 函数\n",
    "out, _ = affine_forward(x, w, b)\n",
    "\n",
    "# 官方提供的正确输出\n",
    "correct_out = np.array([[1.49834967,  1.70660132,  1.91485297],\n",
    "                        [3.25553199,  3.5141327,   3.77273342]])\n",
    "\n",
    "# 比较你的输出与官方正确输出。误差应接近 e-9 或更小。\n",
    "print('测试 affine_forward 函数:')\n",
    "print('差异: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdfab60",
   "metadata": {
    "id": "bfdfab60"
   },
   "source": [
    "# 仿射层：反向传播\n",
    "现在实现 `affine_backward` 函数，并使用数值梯度检查测试你的实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ebdc09",
   "metadata": {
    "id": "42ebdc09"
   },
   "outputs": [],
   "source": [
    "# 测试 affine_backward 函数\n",
    "np.random.seed(231)  # 设置随机种子确保结果可复现\n",
    "\n",
    "# 构造随机输入、权重、偏置和输出梯度\n",
    "x = np.random.randn(10, 2, 3)  # 输入数据\n",
    "w = np.random.randn(6, 5)      # 权重\n",
    "b = np.random.randn(5)         # 偏置\n",
    "dout = np.random.randn(10, 5)  # 上游梯度\n",
    "\n",
    "# 使用数值梯度计算函数计算 dx、dw、db 的数值梯度\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "# 调用 affine_backward 函数计算解析梯度\n",
    "_, cache = affine_forward(x, w, b)\n",
    "dx, dw, db = affine_backward(dout, cache)\n",
    "\n",
    "# 比较数值梯度和解析梯度，误差应接近 e-10 或更小\n",
    "print('测试 affine_backward 函数:')\n",
    "print('dx 误差: ', rel_error(dx_num, dx))\n",
    "print('dw 误差: ', rel_error(dw_num, dw))\n",
    "print('db 误差: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b019e3",
   "metadata": {
    "id": "00b019e3"
   },
   "source": [
    "# ReLU 激活函数：前向传播\n",
    "在 `relu_forward` 函数中实现 ReLU 激活函数的前向传播，并使用以下代码测试你的实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007bbf68",
   "metadata": {
    "id": "007bbf68"
   },
   "outputs": [],
   "source": [
    "# 测试 relu_forward 函数\n",
    "\n",
    "# 构造测试输入\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "# 调用 ReLU 前向传播函数\n",
    "out, _ = relu_forward(x)\n",
    "\n",
    "# 官方提供的正确输出\n",
    "correct_out = np.array([[0.,          0.,          0.,          0.,        ],\n",
    "                        [0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "# 比较你的输出与官方正确输出。误差应在 e-8 数量级。\n",
    "print('测试 relu_forward 函数:')\n",
    "print('差异: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac8a267",
   "metadata": {
    "id": "3ac8a267"
   },
   "source": [
    "# ReLU 激活函数：反向传播\n",
    "现在在 `relu_backward` 函数中实现 ReLU 激活函数的反向传播，并使用数值梯度检查测试你的实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304a1bb2",
   "metadata": {
    "id": "304a1bb2"
   },
   "outputs": [],
   "source": [
    "# 测试 ReLU 反向传播函数\n",
    "np.random.seed(231)  # 设置随机种子确保结果可复现\n",
    "\n",
    "# 构造随机输入和输出梯度\n",
    "x = np.random.randn(10, 10)  # 输入数据\n",
    "dout = np.random.randn(*x.shape)  # 上游梯度\n",
    "\n",
    "# 使用数值梯度计算函数计算 dx 的数值梯度\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "\n",
    "# 调用 ReLU 反向传播函数计算解析梯度\n",
    "_, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "# 比较数值梯度和解析梯度，误差应在 e-12 数量级\n",
    "print('测试 relu_backward 函数:')\n",
    "print('dx 误差: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17b23f3",
   "metadata": {
    "id": "e17b23f3",
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**内联问题 1：**\n",
    "\n",
    "我们只让你实现了 ReLU，但神经网络中可以使用许多不同的激活函数，每个激活函数都有其优缺点。  \n",
    "特别地，激活函数常见的一个问题是在反向传播中梯度消失（或接近消失）。  \n",
    "以下哪些激活函数存在这个问题？  \n",
    "如果你考虑这些函数在一维情况下的表现，哪些类型的输入会导致这种行为？  \n",
    "1. Sigmoid  \n",
    "2. ReLU  \n",
    "3. Leaky ReLU  \n",
    "\n",
    "$\\color{blue}{\\textit{你的答案：}}$ *请在此处填写*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9daf016",
   "metadata": {
    "id": "e9daf016"
   },
   "source": [
    "# “三明治”层\n",
    "神经网络中有一些常见的层组合模式。例如，仿射层通常后面会接一个 ReLU 非线性激活函数。  \n",
    "为了方便使用这些常见模式，我们在文件 `cs231n/layer_utils.py` 中定义了一些便捷函数。\n",
    "\n",
    "目前先查看 `affine_relu_forward` 和 `affine_relu_backward` 函数，并运行以下代码进行数值梯度检查："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30088927",
   "metadata": {
    "id": "30088927"
   },
   "outputs": [],
   "source": [
    "# 测试 affine_relu_forward 和 affine_relu_backward 函数\n",
    "from cs231n.layer_utils import affine_relu_forward, affine_relu_backward\n",
    "\n",
    "# 设置随机种子确保结果可复现\n",
    "np.random.seed(231)\n",
    "\n",
    "# 构造随机输入、权重、偏置和输出梯度\n",
    "x = np.random.randn(2, 3, 4)  # 输入数据\n",
    "w = np.random.randn(12, 10)   # 权重\n",
    "b = np.random.randn(10)       # 偏置\n",
    "dout = np.random.randn(2, 10) # 上游梯度\n",
    "\n",
    "# 调用 affine_relu_forward 函数\n",
    "out, cache = affine_relu_forward(x, w, b)\n",
    "\n",
    "# 调用 affine_relu_backward 函数计算解析梯度\n",
    "dx, dw, db = affine_relu_backward(dout, cache)\n",
    "\n",
    "# 使用数值梯度计算函数计算 dx、dw、db 的数值梯度\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_relu_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_relu_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_relu_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "# 比较数值梯度和解析梯度，相对误差应接近 e-10 或更小\n",
    "print('测试 affine_relu_forward 和 affine_relu_backward:')\n",
    "print('dx 误差: ', rel_error(dx_num, dx))\n",
    "print('dw 误差: ', rel_error(dw_num, dw))\n",
    "print('db 误差: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dd603b",
   "metadata": {
    "id": "37dd603b"
   },
   "source": [
    "# 损失层：Softmax\n",
    "现在在 `cs231n/layers.py` 中的 `softmax_loss` 函数里实现 Softmax 损失及其梯度。  \n",
    "这些实现应与你在 `cs231n/classifiers/softmax.py` 中完成的类似。  \n",
    "其他损失函数（例如 `svm_loss`）也可以以模块化方式实现，但这不是本次作业的要求。\n",
    "\n",
    "你可以通过运行以下代码来验证实现是否正确："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d5269d",
   "metadata": {
    "id": "22d5269d"
   },
   "outputs": [],
   "source": [
    "# 测试 softmax_loss 函数\n",
    "np.random.seed(231)  # 设置随机种子确保结果可复现\n",
    "\n",
    "# 构造随机输入和标签\n",
    "num_classes, num_inputs = 10, 50\n",
    "x = 0.001 * np.random.randn(num_inputs, num_classes)  # 输入数据\n",
    "y = np.random.randint(num_classes, size=num_inputs)   # 随机标签\n",
    "\n",
    "# 使用数值梯度计算函数计算 dx 的数值梯度\n",
    "dx_num = eval_numerical_gradient(lambda x: softmax_loss(x, y)[0], x, verbose=False)\n",
    "\n",
    "# 调用 softmax_loss 函数计算损失和解析梯度\n",
    "loss, dx = softmax_loss(x, y)\n",
    "\n",
    "# 验证 softmax_loss 函数。损失值应接近 2.3，dx 误差应在 e-8 数量级\n",
    "print('\\n测试 softmax_loss:')\n",
    "print('损失: ', loss)\n",
    "print('dx 误差: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5898b95a",
   "metadata": {
    "id": "5898b95a"
   },
   "source": [
    "# 两层网络\n",
    "打开文件 `cs231n/classifiers/fc_net.py` 并完成 `TwoLayerNet` 类的实现。  \n",
    "仔细阅读代码以确保你理解其 API。  \n",
    "你可以运行以下代码单元来测试你的实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a83db9b",
   "metadata": {
    "id": "0a83db9b"
   },
   "outputs": [],
   "source": [
    "# 设置随机种子，确保结果可复现\n",
    "np.random.seed(231)\n",
    "\n",
    "# 构造一个小规模数据集用于快速测试\n",
    "N, D, H, C = 3, 5, 50, 7\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=N)\n",
    "\n",
    "# 初始化 TwoLayerNet，权重初始标准差设为 1e-3\n",
    "std = 1e-3\n",
    "model = TwoLayerNet(input_dim=D, hidden_dim=H, num_classes=C, weight_scale=std)\n",
    "\n",
    "# 测试权重和偏置的初始化是否正确\n",
    "print('测试初始化...')\n",
    "W1_std = abs(model.params['W1'].std() - std)\n",
    "b1 = model.params['b1']\n",
    "W2_std = abs(model.params['W2'].std() - std)\n",
    "b2 = model.params['b2']\n",
    "assert W1_std < std / 10, '第一层权重初始化似乎不正确'\n",
    "assert np.all(b1 == 0), '第一层偏置初始化似乎不正确'\n",
    "assert W2_std < std / 10, '第二层权重初始化似乎不正确'\n",
    "assert np.all(b2 == 0), '第二层偏置初始化似乎不正确'\n",
    "\n",
    "# 测试推理阶段的前向传播\n",
    "print('测试推理阶段的前向传播...')\n",
    "model.params['W1'] = np.linspace(-0.7, 0.3, num=D*H).reshape(D, H)\n",
    "model.params['b1'] = np.linspace(-0.1, 0.9, num=H)\n",
    "model.params['W2'] = np.linspace(-0.3, 0.4, num=H*C).reshape(H, C)\n",
    "model.params['b2'] = np.linspace(-0.9, 0.1, num=C)\n",
    "X = np.linspace(-5.5, 4.5, num=N*D).reshape(D, N).T\n",
    "scores = model.loss(X)\n",
    "\n",
    "# 官方提供的正确输出\n",
    "correct_scores = np.asarray([\n",
    "  [11.53165108, 12.2917344,  13.05181771, 13.81190102, 14.57198434, 15.33206765, 16.09215096],\n",
    "  [12.05769098, 12.74614105, 13.43459113, 14.1230412,  14.81149128, 15.49994135, 16.18839143],\n",
    "  [12.58373087, 13.20054771, 13.81736455, 14.43418138, 15.05099822, 15.66781506, 16.2846319 ]\n",
    "])\n",
    "\n",
    "# 比较误差\n",
    "scores_diff = np.abs(scores - correct_scores).sum()\n",
    "assert scores_diff < 1e-6, '推理阶段前向传播存在问题'\n",
    "\n",
    "# 测试训练损失（无正则化）\n",
    "print('测试训练损失（无正则化）...')\n",
    "y = np.asarray([0, 5, 1])\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 3.4702243556\n",
    "assert abs(loss - correct_loss) < 1e-10, '训练阶段损失存在问题'\n",
    "\n",
    "# 测试正则化损失\n",
    "model.reg = 1.0\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 26.5948426952\n",
    "assert abs(loss - correct_loss) < 1e-10, '正则化损失存在问题'\n",
    "\n",
    "# 数值梯度检查（相对误差应在 e-7 或更低）\n",
    "for reg in [0.0, 0.7]:\n",
    "    print('使用 reg =', reg, '进行数值梯度检查')\n",
    "    model.reg = reg\n",
    "    loss, grads = model.loss(X, y)\n",
    "\n",
    "    for name in sorted(grads):\n",
    "        f = lambda _: model.loss(X, y)[0]\n",
    "        grad_num = eval_numerical_gradient(f, model.params[name], verbose=False)\n",
    "        print('%s 相对误差: %.2e' % (name, rel_error(grad_num, grads[name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adf7b47",
   "metadata": {
    "id": "0adf7b47"
   },
   "source": [
    "# Solver  \n",
    "打开文件 `cs231n/solver.py` 并通读一遍，熟悉其接口。  \n",
    "完成后，使用 `Solver` 实例训练一个 `TwoLayerNet`，使其在验证集上的准确率达到约 **36%**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13472b96",
   "metadata": {
    "id": "13472b96"
   },
   "outputs": [],
   "source": [
    "input_size = 32 * 32 * 3    # 每个样本的展平像素数\n",
    "hidden_size = 50            # 隐藏层神经元数量\n",
    "num_classes = 10            # 类别数（CIFAR-10）\n",
    "\n",
    "# 创建两层全连接网络\n",
    "model = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "solver = None\n",
    "\n",
    "##############################################################################\n",
    "# 代办：使用 Solver 实例训练一个 TwoLayerNet，使其在验证集上的准确率达到约 36%。#\n",
    "##############################################################################\n",
    "\n",
    "##############################################################################\n",
    "#                             代码结束                                       #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdfc7f4",
   "metadata": {
    "id": "ffdfc7f4"
   },
   "source": [
    "# 调试训练过程\n",
    "使用上面给出的默认超参数，你应在验证集上得到约 0.36 的准确率，这并不理想。\n",
    "\n",
    "获取洞察的一种策略是：在优化过程中绘制损失函数曲线，以及训练集和验证集的准确率曲线。\n",
    "\n",
    "另一种策略是：可视化网络第一层学习到的权重。在大多数基于视觉数据训练的神经网络中，第一层的权重在可视化后通常会呈现出明显的结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3649ea",
   "metadata": {
    "id": "1a3649ea"
   },
   "outputs": [],
   "source": [
    "# 运行该单元格，可视化训练损失及训练/验证准确率\n",
    "\n",
    "# 第一幅子图：训练损失\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('训练损失')\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.xlabel('迭代次数')\n",
    "\n",
    "# 第二幅子图：准确率\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('准确率')\n",
    "plt.plot(solver.train_acc_history, '-o', label='训练集')\n",
    "plt.plot(solver.val_acc_history, '-o', label='验证集')\n",
    "plt.plot([0.5] * len(solver.val_acc_history), 'k--')  # 50% 基线\n",
    "plt.xlabel('轮次 (Epoch)')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# 设置整体大小\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6052910",
   "metadata": {
    "id": "a6052910"
   },
   "outputs": [],
   "source": [
    "# 可视化网络权重\n",
    "from cs231n.vis_utils import visualize_grid\n",
    "\n",
    "def show_net_weights(net):\n",
    "    # 取出第一层的权重 W1，并重塑为 (3, 32, 32, hidden_dim)\n",
    "    W1 = net.params['W1']\n",
    "    W1 = W1.reshape(3, 32, 32, -1).transpose(3, 1, 2, 0)  # 调整为 (hidden_dim, 32, 32, 3)\n",
    "    # 使用可视化工具将权重显示为网格图像\n",
    "    plt.imshow(visualize_grid(W1, padding=3).astype('uint8'))\n",
    "    plt.gca().axis('off')  # 关闭坐标轴\n",
    "    plt.show()\n",
    "\n",
    "# 显示当前训练模型的第一层权重\n",
    "show_net_weights(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68386e68",
   "metadata": {
    "id": "68386e68"
   },
   "source": [
    "# 调整超参数\n",
    "\n",
    "**问题出在哪？**  \n",
    "从上面的可视化可以看出：\n",
    "- 损失几乎呈线性下降，这表明学习率可能太低。  \n",
    "- 训练准确率与验证准确率几乎没有差距，说明模型容量偏小，应增大网络规模。  \n",
    "- 反之，如果模型过大，我们通常会看到明显的过拟合，即训练准确率远高于验证准确率。\n",
    "\n",
    "**调参**  \n",
    "调整超参数并培养“它们如何影响最终性能”的直觉，是使用神经网络的重要部分。  \n",
    "下面，你应该尝试不同的超参数组合，包括：\n",
    "- 隐藏层大小  \n",
    "- 学习率  \n",
    "- 训练轮数（epochs）  \n",
    "- 正则化强度  \n",
    "\n",
    "你也可以尝试调节学习率衰减，但使用默认值即可取得较好效果。\n",
    "\n",
    "**预期结果**  \n",
    "目标是在验证集上获得 **>48%** 的准确率。  \n",
    "我们最好的网络在验证集上可达 **>52%**。\n",
    "\n",
    "**实验任务**  \n",
    "本次实验的目标是：  \n",
    "用**全连接神经网络**在 CIFAR-10 上取得尽可能高的结果（52% 可作为参考）。  \n",
    "你可以自由实现自己的想法（例如：用 PCA 降维、添加 Dropout、在 Solver 中加入额外特性等）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d308eb5",
   "metadata": {
    "id": "3d308eb5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_model = None\n",
    "\n",
    "#################################################################################\n",
    "# 代办：在验证集上通过调参找到最优超参数，并将训练好的最佳模型保存到 best_model。   #\n",
    "#                                                                               #\n",
    "# 为帮助调试网络，你可以使用与前面类似的权重可视化；当网络调优后，这些可视化结果     #\n",
    "# 会在质上与之前表现不佳网络的可视化有明显区别。                                   #\n",
    "#                                                                               #\n",
    "# 手动调参虽然有趣，但编写代码自动遍历超参数组合（如之前练习所做）会更高效。         #\n",
    "#################################################################################\n",
    "\n",
    "################################################################################\n",
    "#                              代码结束                                        #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c70f18f",
   "metadata": {
    "id": "6c70f18f"
   },
   "source": [
    "# 测试你的模型！\n",
    "用你训练出的最优模型在验证集和测试集上运行，  \n",
    "你应当在验证集和测试集上都获得 **高于 48%** 的准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666e7589",
   "metadata": {
    "id": "666e7589",
    "test": "val_accuracy"
   },
   "outputs": [],
   "source": [
    "# 在验证集上评估最优模型\n",
    "y_val_pred = np.argmax(best_model.loss(data['X_val']), axis=1)\n",
    "print('验证集准确率: ', (y_val_pred == data['y_val']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d74b7b6",
   "metadata": {
    "id": "0d74b7b6",
    "test": "test_accuracy"
   },
   "outputs": [],
   "source": [
    "# 在测试集上评估最优模型\n",
    "y_test_pred = np.argmax(best_model.loss(data['X_test']), axis=1)\n",
    "print('测试集准确率: ', (y_test_pred == data['y_test']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JlZg7TkKfesG",
   "metadata": {
    "id": "JlZg7TkKfesG"
   },
   "outputs": [],
   "source": [
    "# 保存最优模型\n",
    "best_model.save(\"best_two_layer_net.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8e2ae9",
   "metadata": {
    "id": "ab8e2ae9"
   },
   "source": [
    "**内联问题 2：**\n",
    "\n",
    "现在你已经训练了一个神经网络分类器，可能会发现测试准确率远低于训练准确率。我们可以用哪些方法来缩小这一差距？请选择所有适用的选项。\n",
    "\n",
    "1. 在更大的数据集上训练。  \n",
    "2. 增加隐藏单元数量。  \n",
    "3. 增大正则化强度。  \n",
    "4. 以上都不是。\n",
    "\n",
    "$\\color{blue}{\\textit{你的答案：}}$\n",
    "\n",
    "$\\color{blue}{\\textit{你的解释：}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af96f6b3",
   "metadata": {
    "id": "af96f6b3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
